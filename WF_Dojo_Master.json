{
  "name": "WF_Dojo_Master",
  "nodes": [
    {
      "parameters": {},
      "id": "m-trigger",
      "name": "Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        -1400,
        100
      ]
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-product-types",
      "name": "Get Product Types",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -1360,
        100
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/products/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-products",
      "name": "Get Products",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -1180,
        100
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport os\nimport re\nfrom datetime import datetime, timezone, timedelta\n\nfirst_item = _input.first()\npayload = first_item.json if first_item else None\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\nVALID_STATES = {\n    'new', 'subdomains_running', 'subdomains_done',\n    'nmap_running', 'nmap_done', 'targets_ready',\n    'acu_running', 'done', 'error'\n}\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    return value\n\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return None\n    pattern = re.compile(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", re.DOTALL)\n    m = pattern.search(description)\n    if not m:\n        return None\n    try:\n        data = json.loads(m.group(1).strip())\n        if not isinstance(data, dict):\n            return None\n        return data\n    except Exception:\n        return None\n\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\n\ndef normalize_state(raw_state):\n    state = str(raw_state or 'new')\n    return state if state in VALID_STATES else 'new'\n\n\ndef parse_int(value, default):\n    try:\n        val = int(value)\n        return val if val > 0 else default\n    except Exception:\n        return default\n\n\n\n\ndef parse_bool(value, default=False):\n    if value is None:\n        return default\n    if isinstance(value, bool):\n        return value\n    raw = str(value).strip().lower()\n    if raw in {'1', 'true', 'yes', 'y', 'on'}:\n        return True\n    if raw in {'0', 'false', 'no', 'n', 'off'}:\n        return False\n    return default\n\ndef normalize_node_policy(value):\n    raw = str(value or '').strip().lower()\n    return raw if raw in {'least_loaded', 'weighted'} else 'least_loaded'\ndef parse_ts(value):\n    if not isinstance(value, str) or not value.strip():\n        return None\n    raw = value.strip()\n    if raw.endswith('Z'):\n        raw = raw[:-1] + '+00:00'\n    try:\n        dt = datetime.fromisoformat(raw)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt.astimezone(timezone.utc)\n    except Exception:\n        return None\n\n\nincoming = to_container(payload)\ntrigger_input = to_container(read_key(incoming, 'trigger', {}))\ntrigger_product_type_id = read_key(trigger_input, 'product_type_id')\ntrigger_domain = read_key(trigger_input, 'domain')\n\ntry:\n    trigger_product_type_id = int(trigger_product_type_id) if trigger_product_type_id is not None else None\nexcept Exception:\n    trigger_product_type_id = None\n\npt_window_size = parse_int(read_key(trigger_input, 'pt_window_size', None), parse_int(os.getenv('PT_WINDOW_SIZE'), 1))\nsubdomains_limit = parse_int(read_key(trigger_input, 'subdomains_concurrency', None), parse_int(os.getenv('SUBDOMAINS_CONCURRENCY'), 2))\nnmap_limit = parse_int(read_key(trigger_input, 'nmap_concurrency', None), parse_int(os.getenv('NMAP_CONCURRENCY'), 5))\nsubdomains_stale_minutes = parse_int(read_key(trigger_input, 'subdomains_running_timeout_minutes', None), parse_int(os.getenv('SUBDOMAINS_RUNNING_TIMEOUT_MINUTES'), 60))\n\nproduct_types = to_list(read_key(incoming, 'product_types', []))\nproducts = to_list(read_key(incoming, 'products', []))\n\npt_list = []\nfor pt in product_types:\n    pt_id = read_key(pt, 'id')\n    try:\n        pt_id = int(pt_id)\n    except Exception:\n        continue\n    if trigger_product_type_id is not None and pt_id != trigger_product_type_id:\n        continue\n    pt_list.append(pt)\n\nnow = datetime.now(timezone.utc)\nacu_dispatch_policy = {\n    'fairness': 'round_robin_by_pt',\n    'node_selection': normalize_node_policy(os.getenv('ACUNETIX_NODE_SELECTION_POLICY')),\n    'sticky_assignment': parse_bool(os.getenv('ACUNETIX_STICKY_ASSIGNMENT', 'true'), True),\n}\nsubdomains_running_count = 0\nrecovered_stale_subdomains = 0\n\nprepared_pts = []\nfor pt in sorted(pt_list, key=lambda x: int(read_key(x, 'id', 0) or 0)):\n    pt_id = int(read_key(pt, 'id'))\n    original_description = read_key(pt, 'description', '')\n    state_obj = parse_state(original_description) or {}\n    state = normalize_state(state_obj.get('state'))\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n\n    if state == 'subdomains_running':\n        last_update_dt = parse_ts(state_obj.get('last_update'))\n        stale = False\n        if last_update_dt is None:\n            stale = True\n        else:\n            stale = now - last_update_dt > timedelta(minutes=subdomains_stale_minutes)\n\n        if stale:\n            recovered_stale_subdomains += 1\n            retry_count += 1\n            state = 'error'\n            updated = {\n                'version': 1,\n                'state': 'error',\n                'counters': counters,\n                'last_update': now.isoformat(),\n                'retry_count': retry_count,\n                'last_error': f'Recovered stale subdomains_running on restart (timeout {subdomains_stale_minutes}m).',\n            }\n            prepared_pts.append({'pt': pt, 'state': state, 'state_obj': updated, 'original_description': original_description})\n            continue\n\n        subdomains_running_count += 1\n\n    prepared_pts.append({'pt': pt, 'state': state, 'state_obj': state_obj, 'original_description': original_description})\n\nsubdomains_slots = max(0, subdomains_limit - subdomains_running_count)\n\neligible = []\nfor row in prepared_pts:\n    if row['state'] in ('new', 'error', 'subdomains_done', 'nmap_done', 'targets_ready'):\n        eligible.append(row)\n\nselected_rows = eligible[:pt_window_size]\nselected_ids = set(int(read_key(row['pt'], 'id')) for row in selected_rows)\n\nqueue_wf_a_subdomains_pt = []\nqueue_wf_b_nmap_product = []\nqueue_wf_c_targets_for_pt = []\nqueue_wf_d_pt_acunetixscan = []\nstate_updates = []\n\nfor row in prepared_pts:\n    if row.get('state_obj', {}).get('state') == 'error' and row['state'] == 'error' and 'Recovered stale subdomains_running' in str(row['state_obj'].get('last_error', '')):\n        pt_id = int(read_key(row['pt'], 'id'))\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(row['original_description'], row['state_obj']),\n            'state': 'error',\n        })\n\nfor row in selected_rows:\n    pt = row['pt']\n    pt_id = int(read_key(pt, 'id'))\n    pt_name = read_key(pt, 'name')\n    original_description = row['original_description']\n    state_obj = row['state_obj'] if isinstance(row['state_obj'], dict) else {}\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    current_state = row['state']\n\n    next_state = None\n    queue_name = None\n    if current_state in ('new', 'error'):\n        if subdomains_slots > 0:\n            next_state = 'subdomains_running'\n            queue_name = 'a'\n            subdomains_slots -= 1\n    elif current_state in ('subdomains_done', 'nmap_running'):\n        queue_name = 'b'\n    elif current_state == 'nmap_done':\n        next_state = 'targets_ready'\n        queue_name = 'c'\n    elif current_state == 'targets_ready':\n        next_state = 'acu_running'\n        queue_name = 'd'\n\n    if queue_name == 'a':\n        queue_wf_a_subdomains_pt.append({'product_type_id': pt_id, 'product_type_name': pt_name, 'domain': trigger_domain})\n    elif queue_name == 'b':\n        jobs = []\n        for product in products:\n            try:\n                product_pt = int(read_key(product, 'prod_type'))\n                product_id = int(read_key(product, 'id'))\n            except Exception:\n                continue\n            if product_pt != pt_id:\n                continue\n            jobs.append({\n                'product_id': product_id,\n                'product_name': read_key(product, 'name'),\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain\n            })\n        jobs = sorted(jobs, key=lambda p: int(read_key(p, 'product_id', 0) or 0))\n\n        nmap_total = int(counters.get('nmap_total', 0) or 0)\n        if nmap_total <= 0:\n            nmap_total = len(jobs)\n        nmap_done = int(counters.get('nmap_done', 0) or 0)\n        nmap_failed = int(counters.get('nmap_failed', 0) or 0)\n\n        processed = max(0, min(nmap_total, nmap_done + nmap_failed))\n        pending_jobs = jobs[processed:]\n        batch = pending_jobs[:nmap_limit]\n\n        counters['nmap_total'] = nmap_total\n        counters['nmap_done'] = nmap_done\n        counters['nmap_failed'] = nmap_failed\n\n        for job in batch:\n            queue_wf_b_nmap_product.append({\n                **job,\n                'nmap_total': nmap_total,\n                'nmap_done_initial': nmap_done,\n                'nmap_failed_initial': nmap_failed,\n                'nmap_state_description': original_description,\n            })\n\n        if nmap_total == 0 or processed >= nmap_total:\n            next_state = 'nmap_done'\n        elif batch:\n            next_state = 'nmap_running'\n    elif queue_name == 'c':\n        queue_wf_c_targets_for_pt.append({'product_type_id': pt_id, 'product_type_name': pt_name, 'domain': trigger_domain})\n    elif queue_name == 'd':\n        queue_wf_d_pt_acunetixscan.append({'product_type_id': pt_id, 'product_type_name': pt_name, 'domain': trigger_domain, 'dispatch_policy': acu_dispatch_policy})\n\n    if next_state:\n        stage_key = {\n            'subdomains_running': 'subdomains_runs',\n            'nmap_running': 'nmap_runs',\n            'targets_ready': 'targets_runs',\n            'acu_running': 'acu_runs',\n        }.get(next_state)\n        counters[stage_key] = int(counters.get(stage_key, 0) or 0) + 1\n        updated = {\n            'version': 1,\n            'state': next_state,\n            'counters': counters,\n            'last_update': now.isoformat(),\n            'retry_count': retry_count,\n            'last_error': None if next_state == 'subdomains_running' else state_obj.get('last_error'),\n        }\n        if next_state == 'acu_running':\n            updated['acu_dispatch_policy'] = acu_dispatch_policy\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(original_description, updated),\n            'state': next_state,\n        })\n\nreturn [{\n    'json': {\n        'run_subdomains': len(queue_wf_a_subdomains_pt) > 0,\n        'run_nmap': len(queue_wf_b_nmap_product) > 0,\n        'run_targets': len(queue_wf_c_targets_for_pt) > 0,\n        'run_acunetix': len(queue_wf_d_pt_acunetixscan) > 0,\n        'queue_wf_a_subdomains_pt': queue_wf_a_subdomains_pt,\n        'queue_wf_b_nmap_product': queue_wf_b_nmap_product,\n        'queue_wf_c_targets_for_pt': queue_wf_c_targets_for_pt,\n        'queue_wf_d_pt_acunetixscan': queue_wf_d_pt_acunetixscan,\n        'state_updates': state_updates,\n        'trigger_product_type_id': trigger_product_type_id,\n        'trigger_domain': trigger_domain,\n        'pt_window_size': pt_window_size,\n        'selected_pt_ids': sorted(list(selected_ids)),\n        'subdomains_concurrency': subdomains_limit,\n        'subdomains_running_now': subdomains_running_count,\n        'subdomains_slots_available': max(0, subdomains_limit - subdomains_running_count),\n        'subdomains_stale_recovered': recovered_stale_subdomains,\n        'subdomains_running_timeout_minutes': subdomains_stale_minutes,\n        'nmap_concurrency': nmap_limit,\n        'stage_barriers': {\n            'subdomains_done_after_all_jobs': True,\n            'nmap_done_after_all_jobs': True,\n        },\n        'acu_dispatch_policy': acu_dispatch_policy,\n    }\n}]"
      },
      "id": "m-plan",
      "name": "Build Plan (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -950,
        100
      ]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_subdomains}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-subdomains",
      "name": "Need WF_A_Subdomains_PT?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -760,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_A_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-a",
      "name": "Run WF_A_Subdomains_PT",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        -460,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_nmap}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-nmap",
      "name": "Need WF_B_Nmap_Product?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -340,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_B_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-b",
      "name": "Run WF_B_Nmap_Product",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        -40,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_targets}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-targets",
      "name": "Need WF_C_Targets_For_PT?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        60,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_C_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-c",
      "name": "Run WF_C_Targets_For_PT",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        360,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_acunetix}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-acu",
      "name": "Need WF_D_PT_AcunetixScan?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        460,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_D_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-d",
      "name": "Run WF_D_PT_AcunetixScan",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        670,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_a_subdomains_pt', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-a",
      "name": "Expand WF_A Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -660,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_b_nmap_product', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['product_id'] = read_key(row, 'product_id')\n    item['product_name'] = read_key(row, 'product_name')\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['domain'] = read_key(row, 'domain')\n    item['nmap_state_description'] = read_key(row, 'nmap_state_description')\n    item['nmap_failed_initial'] = read_key(row, 'nmap_failed_initial')\n    item['nmap_done_initial'] = read_key(row, 'nmap_done_initial')\n    item['nmap_total'] = read_key(row, 'nmap_total')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-b",
      "name": "Expand WF_B Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -240,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_c_targets_for_pt', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-c",
      "name": "Expand WF_C Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        160,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_d_pt_acunetixscan', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-d",
      "name": "Expand WF_D Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        560,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\nitem = first_item.json if first_item else {}\nif not isinstance(item, dict):\n    item = {}\nreturn [{'json': {'trigger': item}}]"
      },
      "id": "m-prepare-trigger",
      "name": "Prepare Trigger Input (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1170,
        -80
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'product_types': aggregated}}]"
      },
      "id": "m-prepare-pt",
      "name": "Prepare Product Types (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -960,
        -80
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'products': aggregated}}]"
      },
      "id": "m-prepare-products",
      "name": "Prepare Products (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -740,
        280
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-trigger-pt",
      "name": "Merge Trigger + Product Types",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        -740,
        -80
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-build-input",
      "name": "Merge Build Plan Input",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        -520,
        100
      ]
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-product-types-after-a",
      "name": "Get Product Types After WF_A",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -300,
        -260
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/products/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-products-after-a",
      "name": "Get Products After WF_A",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -300,
        -120
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'product_types': aggregated}}]"
      },
      "id": "m-prepare-pt-after-a",
      "name": "Prepare Product Types After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -260
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'products': aggregated}}]"
      },
      "id": "m-prepare-products-after-a",
      "name": "Prepare Products After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -120
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-trigger-pt-after-a",
      "name": "Merge Trigger + Product Types After WF_A",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        150,
        -220
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-rebuild-input",
      "name": "Merge Rebuild Plan Input",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        340,
        -140
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport os\nimport re\nfrom datetime import datetime, timezone, timedelta\n\nfirst_item = _input.first()\npayload = first_item.json if first_item else None\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\nVALID_STATES = {\n    'new', 'subdomains_running', 'subdomains_done',\n    'nmap_running', 'nmap_done', 'targets_ready',\n    'acu_running', 'done', 'error'\n}\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    return value\n\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return None\n    pattern = re.compile(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", re.DOTALL)\n    m = pattern.search(description)\n    if not m:\n        return None\n    try:\n        data = json.loads(m.group(1).strip())\n        if not isinstance(data, dict):\n            return None\n        return data\n    except Exception:\n        return None\n\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\n\ndef normalize_state(raw_state):\n    state = str(raw_state or 'new')\n    return state if state in VALID_STATES else 'new'\n\n\ndef parse_int(value, default):\n    try:\n        val = int(value)\n        return val if val > 0 else default\n    except Exception:\n        return default\n\n\n\n\ndef parse_bool(value, default=False):\n    if value is None:\n        return default\n    if isinstance(value, bool):\n        return value\n    raw = str(value).strip().lower()\n    if raw in {'1', 'true', 'yes', 'y', 'on'}:\n        return True\n    if raw in {'0', 'false', 'no', 'n', 'off'}:\n        return False\n    return default\n\ndef normalize_node_policy(value):\n    raw = str(value or '').strip().lower()\n    return raw if raw in {'least_loaded', 'weighted'} else 'least_loaded'\ndef parse_ts(value):\n    if not isinstance(value, str) or not value.strip():\n        return None\n    raw = value.strip()\n    if raw.endswith('Z'):\n        raw = raw[:-1] + '+00:00'\n    try:\n        dt = datetime.fromisoformat(raw)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt.astimezone(timezone.utc)\n    except Exception:\n        return None\n\n\nincoming = to_container(payload)\ntrigger_input = to_container(read_key(incoming, 'trigger', {}))\ntrigger_product_type_id = read_key(trigger_input, 'product_type_id')\ntrigger_domain = read_key(trigger_input, 'domain')\n\ntry:\n    trigger_product_type_id = int(trigger_product_type_id) if trigger_product_type_id is not None else None\nexcept Exception:\n    trigger_product_type_id = None\n\npt_window_size = parse_int(read_key(trigger_input, 'pt_window_size', None), parse_int(os.getenv('PT_WINDOW_SIZE'), 1))\nsubdomains_limit = parse_int(read_key(trigger_input, 'subdomains_concurrency', None), parse_int(os.getenv('SUBDOMAINS_CONCURRENCY'), 2))\nnmap_limit = parse_int(read_key(trigger_input, 'nmap_concurrency', None), parse_int(os.getenv('NMAP_CONCURRENCY'), 5))\nsubdomains_stale_minutes = parse_int(read_key(trigger_input, 'subdomains_running_timeout_minutes', None), parse_int(os.getenv('SUBDOMAINS_RUNNING_TIMEOUT_MINUTES'), 60))\n\nproduct_types = to_list(read_key(incoming, 'product_types', []))\nproducts = to_list(read_key(incoming, 'products', []))\n\npt_list = []\nfor pt in product_types:\n    pt_id = read_key(pt, 'id')\n    try:\n        pt_id = int(pt_id)\n    except Exception:\n        continue\n    if trigger_product_type_id is not None and pt_id != trigger_product_type_id:\n        continue\n    pt_list.append(pt)\n\nnow = datetime.now(timezone.utc)\nacu_dispatch_policy = {\n    'fairness': 'round_robin_by_pt',\n    'node_selection': normalize_node_policy(os.getenv('ACUNETIX_NODE_SELECTION_POLICY')),\n    'sticky_assignment': parse_bool(os.getenv('ACUNETIX_STICKY_ASSIGNMENT', 'true'), True),\n}\nsubdomains_running_count = 0\nrecovered_stale_subdomains = 0\n\nprepared_pts = []\nfor pt in sorted(pt_list, key=lambda x: int(read_key(x, 'id', 0) or 0)):\n    pt_id = int(read_key(pt, 'id'))\n    original_description = read_key(pt, 'description', '')\n    state_obj = parse_state(original_description) or {}\n    state = normalize_state(state_obj.get('state'))\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n\n    if state == 'subdomains_running':\n        last_update_dt = parse_ts(state_obj.get('last_update'))\n        stale = False\n        if last_update_dt is None:\n            stale = True\n        else:\n            stale = now - last_update_dt > timedelta(minutes=subdomains_stale_minutes)\n\n        if stale:\n            recovered_stale_subdomains += 1\n            retry_count += 1\n            state = 'error'\n            updated = {\n                'version': 1,\n                'state': 'error',\n                'counters': counters,\n                'last_update': now.isoformat(),\n                'retry_count': retry_count,\n                'last_error': f'Recovered stale subdomains_running on restart (timeout {subdomains_stale_minutes}m).',\n            }\n            prepared_pts.append({'pt': pt, 'state': state, 'state_obj': updated, 'original_description': original_description})\n            continue\n\n        subdomains_running_count += 1\n\n    prepared_pts.append({'pt': pt, 'state': state, 'state_obj': state_obj, 'original_description': original_description})\n\nsubdomains_slots = max(0, subdomains_limit - subdomains_running_count)\n\neligible = []\nfor row in prepared_pts:\n    if row['state'] in ('new', 'error', 'subdomains_done', 'nmap_done', 'targets_ready'):\n        eligible.append(row)\n\nselected_rows = eligible[:pt_window_size]\nselected_ids = set(int(read_key(row['pt'], 'id')) for row in selected_rows)\n\nqueue_wf_a_subdomains_pt = []\nqueue_wf_b_nmap_product = []\nqueue_wf_c_targets_for_pt = []\nqueue_wf_d_pt_acunetixscan = []\nstate_updates = []\n\nfor row in prepared_pts:\n    if row.get('state_obj', {}).get('state') == 'error' and row['state'] == 'error' and 'Recovered stale subdomains_running' in str(row['state_obj'].get('last_error', '')):\n        pt_id = int(read_key(row['pt'], 'id'))\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(row['original_description'], row['state_obj']),\n            'state': 'error',\n        })\n\nfor row in selected_rows:\n    pt = row['pt']\n    pt_id = int(read_key(pt, 'id'))\n    pt_name = read_key(pt, 'name')\n    original_description = row['original_description']\n    state_obj = row['state_obj'] if isinstance(row['state_obj'], dict) else {}\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    current_state = row['state']\n\n    next_state = None\n    queue_name = None\n    if current_state in ('new', 'error'):\n        if subdomains_slots > 0:\n            next_state = 'subdomains_running'\n            queue_name = 'a'\n            subdomains_slots -= 1\n    elif current_state in ('subdomains_done', 'nmap_running'):\n        queue_name = 'b'\n    elif current_state == 'nmap_done':\n        next_state = 'targets_ready'\n        queue_name = 'c'\n    elif current_state == 'targets_ready':\n        next_state = 'acu_running'\n        queue_name = 'd'\n\n    if queue_name == 'a':\n        queue_wf_a_subdomains_pt.append({'product_type_id': pt_id, 'product_type_name': pt_name, 'domain': trigger_domain})\n    elif queue_name == 'b':\n        jobs = []\n        for product in products:\n            try:\n                product_pt = int(read_key(product, 'prod_type'))\n                product_id = int(read_key(product, 'id'))\n            except Exception:\n                continue\n            if product_pt != pt_id:\n                continue\n            jobs.append({\n                'product_id': product_id,\n                'product_name': read_key(product, 'name'),\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain\n            })\n        jobs = sorted(jobs, key=lambda p: int(read_key(p, 'product_id', 0) or 0))\n\n        nmap_total = int(counters.get('nmap_total', 0) or 0)\n        if nmap_total <= 0:\n            nmap_total = len(jobs)\n        nmap_done = int(counters.get('nmap_done', 0) or 0)\n        nmap_failed = int(counters.get('nmap_failed', 0) or 0)\n\n        processed = max(0, min(nmap_total, nmap_done + nmap_failed))\n        pending_jobs = jobs[processed:]\n        batch = pending_jobs[:nmap_limit]\n\n        counters['nmap_total'] = nmap_total\n        counters['nmap_done'] = nmap_done\n        counters['nmap_failed'] = nmap_failed\n\n        for job in batch:\n            queue_wf_b_nmap_product.append({\n                **job,\n                'nmap_total': nmap_total,\n                'nmap_done_initial': nmap_done,\n                'nmap_failed_initial': nmap_failed,\n                'nmap_state_description': original_description,\n            })\n\n        if nmap_total == 0 or processed >= nmap_total:\n            next_state = 'nmap_done'\n        elif batch:\n            next_state = 'nmap_running'\n    elif queue_name == 'c':\n        queue_wf_c_targets_for_pt.append({'product_type_id': pt_id, 'product_type_name': pt_name, 'domain': trigger_domain})\n    elif queue_name == 'd':\n        queue_wf_d_pt_acunetixscan.append({'product_type_id': pt_id, 'product_type_name': pt_name, 'domain': trigger_domain, 'dispatch_policy': acu_dispatch_policy})\n\n    if next_state:\n        stage_key = {\n            'subdomains_running': 'subdomains_runs',\n            'nmap_running': 'nmap_runs',\n            'targets_ready': 'targets_runs',\n            'acu_running': 'acu_runs',\n        }.get(next_state)\n        counters[stage_key] = int(counters.get(stage_key, 0) or 0) + 1\n        updated = {\n            'version': 1,\n            'state': next_state,\n            'counters': counters,\n            'last_update': now.isoformat(),\n            'retry_count': retry_count,\n            'last_error': None if next_state == 'subdomains_running' else state_obj.get('last_error'),\n        }\n        if next_state == 'acu_running':\n            updated['acu_dispatch_policy'] = acu_dispatch_policy\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(original_description, updated),\n            'state': next_state,\n        })\n\nreturn [{\n    'json': {\n        'run_subdomains': len(queue_wf_a_subdomains_pt) > 0,\n        'run_nmap': len(queue_wf_b_nmap_product) > 0,\n        'run_targets': len(queue_wf_c_targets_for_pt) > 0,\n        'run_acunetix': len(queue_wf_d_pt_acunetixscan) > 0,\n        'queue_wf_a_subdomains_pt': queue_wf_a_subdomains_pt,\n        'queue_wf_b_nmap_product': queue_wf_b_nmap_product,\n        'queue_wf_c_targets_for_pt': queue_wf_c_targets_for_pt,\n        'queue_wf_d_pt_acunetixscan': queue_wf_d_pt_acunetixscan,\n        'state_updates': state_updates,\n        'trigger_product_type_id': trigger_product_type_id,\n        'trigger_domain': trigger_domain,\n        'pt_window_size': pt_window_size,\n        'selected_pt_ids': sorted(list(selected_ids)),\n        'subdomains_concurrency': subdomains_limit,\n        'subdomains_running_now': subdomains_running_count,\n        'subdomains_slots_available': max(0, subdomains_limit - subdomains_running_count),\n        'subdomains_stale_recovered': recovered_stale_subdomains,\n        'subdomains_running_timeout_minutes': subdomains_stale_minutes,\n        'nmap_concurrency': nmap_limit,\n        'stage_barriers': {\n            'subdomains_done_after_all_jobs': True,\n            'nmap_done_after_all_jobs': True,\n        },\n        'acu_dispatch_policy': acu_dispatch_policy,\n    }\n}]"
      },
      "id": "m-rebuild-plan",
      "name": "Rebuild Plan After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        560,
        -140
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else {}\nupdates = payload.get('state_updates') if isinstance(payload, dict) else []\nif not isinstance(updates, list):\n    updates = []\nout = []\nfor u in updates:\n    if not isinstance(u, dict):\n        continue\n    pt_id = u.get('product_type_id')\n    desc = u.get('description')\n    if pt_id is None or desc is None:\n        continue\n    out.append({'json': {'product_type_id': pt_id, 'description': desc}})\nreturn out"
      },
      "id": "m-expand-state-updates",
      "name": "Expand PT State Updates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -520,
        220
      ]
    },
    {
      "parameters": {
        "method": "PATCH",
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/' + $json.product_type_id + '/' }}",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { description: $json.description } }}",
        "options": {}
      },
      "id": "m-patch-pt-state",
      "name": "Patch PT State Description",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -260,
        220
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport re\nfrom datetime import datetime, timezone\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\n\ndef read_key(container, key, default=None):\n    try:\n        val = container.get(key)\n        return default if val is None else val\n    except Exception:\n        try:\n            val = container[key]\n            return default if val is None else val\n        except Exception:\n            return default\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return {}\n    m = re.search(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", description, re.DOTALL)\n    if not m:\n        return {}\n    try:\n        state = json.loads(m.group(1).strip())\n        return state if isinstance(state, dict) else {}\n    except Exception:\n        return {}\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\nitems = _input.all()\nif not items:\n    return []\n\nnow = datetime.now(timezone.utc).isoformat()\nby_pt = {}\nfor item in items:\n    row = item.json if isinstance(item.json, dict) else {}\n    try:\n        pt_id = int(read_key(row, 'product_type_id'))\n    except Exception:\n        continue\n\n    agg = by_pt.get(pt_id)\n    if agg is None:\n        agg = {\n            'description': read_key(row, 'nmap_state_description', ''),\n            'nmap_total': int(read_key(row, 'nmap_total', 0) or 0),\n            'nmap_done': int(read_key(row, 'nmap_done_initial', 0) or 0),\n            'nmap_failed': int(read_key(row, 'nmap_failed_initial', 0) or 0),\n            'done_inc': 0,\n            'failed_inc': 0,\n        }\n        by_pt[pt_id] = agg\n\n    status = str(read_key(row, 'workflow_status', 'ok') or 'ok')\n    if status == 'ok':\n        agg['done_inc'] += 1\n    else:\n        agg['failed_inc'] += 1\n\nout = []\nfor pt_id, agg in by_pt.items():\n    state_obj = parse_state(agg['description'])\n    counters = read_key(state_obj, 'counters', {})\n    if not isinstance(counters, dict):\n        counters = {}\n\n    nmap_total = agg['nmap_total'] if agg['nmap_total'] > 0 else int(counters.get('nmap_total', 0) or 0)\n    nmap_done = int(agg['nmap_done']) + int(agg['done_inc'])\n    nmap_failed = int(agg['nmap_failed']) + int(agg['failed_inc'])\n\n    counters['nmap_total'] = nmap_total\n    counters['nmap_done'] = nmap_done\n    counters['nmap_failed'] = nmap_failed\n\n    processed = nmap_done + nmap_failed\n    next_state = 'nmap_done' if nmap_total == 0 or processed >= nmap_total else 'nmap_running'\n\n    updated_state = {\n        'version': 1,\n        'state': next_state,\n        'counters': counters,\n        'last_update': now,\n        'retry_count': int(read_key(state_obj, 'retry_count', 0) or 0),\n        'last_error': read_key(state_obj, 'last_error'),\n    }\n\n    out.append({'json': {\n        'product_type_id': pt_id,\n        'state': next_state,\n        'description': build_description(agg['description'], updated_state),\n        'workflow_status': 'ok',\n    }})\n\nreturn out\n"
      },
      "name": "Summarize WF_B Results (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1904,
        432
      ],
      "id": "m-summarize-b"
    }
  ],
  "connections": {
    "Trigger": {
      "main": [
        [
          {
            "node": "Get Product Types",
            "type": "main",
            "index": 0
          },
          {
            "node": "Prepare Trigger Input (Python)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Products",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Products": {
      "main": [
        [
          {
            "node": "Prepare Products (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Plan (Python)": {
      "main": [
        [
          {
            "node": "Expand PT State Updates (Python)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Need WF_A_Subdomains_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_A_Subdomains_PT?": {
      "main": [
        [
          {
            "node": "Expand WF_A Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_B_Nmap_Product?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_A_Subdomains_PT": {
      "main": [
        [
          {
            "node": "Get Product Types After WF_A",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Products After WF_A",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_B_Nmap_Product?": {
      "main": [
        [
          {
            "node": "Expand WF_B Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_C_Targets_For_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_B_Nmap_Product": {
      "main": [
        [
          {
            "node": "Summarize WF_B Results (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_C_Targets_For_PT?": {
      "main": [
        [
          {
            "node": "Expand WF_C Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_D_PT_AcunetixScan?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_C_Targets_For_PT": {
      "main": [
        [
          {
            "node": "Need WF_D_PT_AcunetixScan?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_D_PT_AcunetixScan?": {
      "main": [
        [
          {
            "node": "Expand WF_D Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "Get Product Types": {
      "main": [
        [
          {
            "node": "Prepare Product Types (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_A Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_A_Subdomains_PT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_B Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_B_Nmap_Product",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_C Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_C_Targets_For_PT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_D Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_D_PT_AcunetixScan",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Product Types (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare Trigger Input (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Products (Python)": {
      "main": [
        [
          {
            "node": "Merge Build Plan Input",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Trigger + Product Types": {
      "main": [
        [
          {
            "node": "Merge Build Plan Input",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge Trigger + Product Types After WF_A",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Build Plan Input": {
      "main": [
        [
          {
            "node": "Build Plan (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Product Types After WF_A": {
      "main": [
        [
          {
            "node": "Prepare Product Types After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Products After WF_A": {
      "main": [
        [
          {
            "node": "Prepare Products After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Product Types After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types After WF_A",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare Products After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Merge Rebuild Plan Input",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Trigger + Product Types After WF_A": {
      "main": [
        [
          {
            "node": "Merge Rebuild Plan Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Rebuild Plan Input": {
      "main": [
        [
          {
            "node": "Rebuild Plan After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Rebuild Plan After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Expand PT State Updates (Python)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Need WF_B_Nmap_Product?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand PT State Updates (Python)": {
      "main": [
        [
          {
            "node": "Patch PT State Description",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Summarize WF_B Results (Python)": {
      "main": [
        [
          {
            "node": "Patch PT State Description",
            "type": "main",
            "index": 0
          },
          {
            "node": "Need WF_C_Targets_For_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "tags": []
}
