{
  "name": "WF_Dojo_Master",
  "nodes": [
    {
      "parameters": {},
      "id": "m-trigger",
      "name": "Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        -1400,
        100
      ]
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-product-types",
      "name": "Get Product Types",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -1360,
        100
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/products/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-products",
      "name": "Get Products",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -1180,
        100
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport os\nimport re\nfrom datetime import datetime, timezone, timedelta\n\nMAPPING_DB_PATH = (os.environ.get('ACUNETIX_MAPPING_DB') or '/tmp/acunetix_mapping_store.sqlite3').strip()\n\nfirst_item = _input.first()\npayload = first_item.json if first_item else None\n\nplan_input_source = str((payload or {}).get('plan_input_source', 'normal') or 'normal')\nPLANNER_VERSION = 'dojo-plan-builder-v1'\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\nVALID_STATES = {\n    'new', 'subdomains_running', 'subdomains_done',\n    'nmap_running', 'nmap_done', 'targets_ready',\n    'acu_running', 'done', 'error'\n}\n\nTARGETS_STAGE_CONTRACT_FIELDS = [\n    'pt_id', 'product_type_id', 'product_type_name', 'domain', 'stage',\n    'selected_acu_node', 'acunetix_endpoint', 'acunetix_node_name',\n    'job_metadata',\n]\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    return value\n\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return None\n    pattern = re.compile(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", re.DOTALL)\n    m = pattern.search(description)\n    if not m:\n        return None\n    try:\n        data = json.loads(m.group(1).strip())\n        if not isinstance(data, dict):\n            return None\n        return data\n    except Exception:\n        return None\n\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\n\ndef normalize_state(raw_state):\n    state = str(raw_state or 'new')\n    return state if state in VALID_STATES else 'new'\n\n\ndef parse_int(value, default):\n    try:\n        val = int(value)\n        return val if val > 0 else default\n    except Exception:\n        return default\n\n\ndef parse_bool(value, default=False):\n    if value is None:\n        return default\n    if isinstance(value, bool):\n        return value\n    raw = str(value).strip().lower()\n    if raw in {'1', 'true', 'yes', 'y', 'on'}:\n        return True\n    if raw in {'0', 'false', 'no', 'n', 'off'}:\n        return False\n    return default\n\n\ndef normalize_node_policy(value):\n    raw = str(value or '').strip().lower()\n    return raw if raw in {'least_loaded', 'weighted'} else 'least_loaded'\n\n\ndef normalize_targets_fallback_policy(value):\n    raw = str(value or '').strip().lower().replace('-', '_')\n    return raw if raw in {'least_loaded', 'sticky'} else 'sticky'\n\n\n\ndef nodes_equal(lhs, rhs):\n    if not isinstance(lhs, dict) or not isinstance(rhs, dict):\n        return False\n    return str(lhs.get('endpoint') or '').strip().rstrip('/') == str(rhs.get('endpoint') or '').strip().rstrip('/') and str(lhs.get('name') or '').strip() == str(rhs.get('name') or '').strip()\n\n\ndef init_db(conn):\n    conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS acunetix_mapping (\n            pt_id TEXT NOT NULL,\n            product_id TEXT NOT NULL,\n            selected_acu_node TEXT,\n            target_id TEXT,\n            updated_at TEXT NOT NULL,\n            PRIMARY KEY (pt_id, product_id)\n        )\n    \"\"\")\n\n\ndef load_pt_node_mapping(path):\n    out = {}\n    if not path:\n        return out\n    try:\n        conn = sqlite3.connect(path, timeout=30)\n        with conn:\n            init_db(conn)\n            for pt_id, selected_raw, updated_at in conn.execute(\"SELECT pt_id, selected_acu_node, updated_at FROM acunetix_mapping WHERE selected_acu_node IS NOT NULL\"):\n                try:\n                    node = json.loads(selected_raw) if selected_raw else None\n                except Exception:\n                    node = None\n                if not isinstance(node, dict):\n                    continue\n                prev = out.get(str(pt_id))\n                if (not prev) or str(updated_at or '') >= str(prev.get('_updated_at') or ''):\n                    node['_updated_at'] = str(updated_at or '')\n                    out[str(pt_id)] = node\n        conn.close()\n    except Exception:\n        return {}\n    for row in out.values():\n        row.pop('_updated_at', None)\n    return out\n\n\ndef persist_pt_node_mapping(path, updates):\n    if not path or not isinstance(updates, dict) or not updates:\n        return 0\n    count = 0\n    conn = sqlite3.connect(path, timeout=30)\n    with conn:\n        init_db(conn)\n        ts = datetime.now(timezone.utc).isoformat()\n        for pt_id, node in updates.items():\n            normalized = normalize_selected_node(node)\n            if not normalized:\n                continue\n            conn.execute(\"INSERT INTO acunetix_mapping(pt_id,product_id,selected_acu_node,target_id,updated_at) VALUES(?,?,?,?,?) ON CONFLICT(pt_id,product_id) DO UPDATE SET selected_acu_node=excluded.selected_acu_node,updated_at=excluded.updated_at\", (str(pt_id), '__pt_binding__', json.dumps({'name': normalized.get('name'), 'endpoint': normalized.get('endpoint'), 'weight': normalized.get('weight',1)}, ensure_ascii=False), None, ts))\n            count += 1\n    conn.close()\n    return count\n\n\ndef normalize_selected_node(raw):\n    if not isinstance(raw, dict):\n        return None\n    endpoint = str(raw.get('endpoint') or '').strip().rstrip('/')\n    token = str(raw.get('api_key') or raw.get('token') or '').strip()\n    if not endpoint or not token:\n        return None\n    return {\n        'endpoint': endpoint,\n        'api_key': token,\n        'token': token,\n        'name': str(raw.get('name') or 'acu-selected').strip() or 'acu-selected',\n        'weight': parse_int(raw.get('weight', 1), 1),\n    }\n\n\ndef normalize_node_reference(raw):\n    if not isinstance(raw, dict):\n        return None\n    endpoint = str(raw.get('endpoint') or '').strip().rstrip('/')\n    name = str(raw.get('name') or '').strip()\n    if not endpoint and not name:\n        return None\n    return {\n        'endpoint': endpoint,\n        'name': name,\n    }\n\n\ndef resolve_node_from_reference(raw, nodes):\n    ref = normalize_node_reference(raw)\n    if not ref:\n        return None\n    endpoint = ref.get('endpoint')\n    name = ref.get('name')\n    for node in nodes:\n        if endpoint and str(node.get('endpoint') or '').strip().rstrip('/') == endpoint:\n            return normalize_selected_node(node)\n    for node in nodes:\n        if name and str(node.get('name') or '').strip() == name:\n            return normalize_selected_node(node)\n    return None\n\n\ndef load_acunetix_nodes_from_env():\n    nodes = []\n    raw = (os.environ.get('ACUNETIX_INSTANCES_JSON') or '').strip()\n    if raw:\n        try:\n            arr = json.loads(raw)\n            if isinstance(arr, list):\n                for idx, item in enumerate(arr):\n                    node = normalize_selected_node(item)\n                    if node:\n                        if not str(node.get('name') or '').strip():\n                            node['name'] = f'acu-{idx + 1}'\n                        nodes.append(node)\n        except Exception:\n            nodes = []\n\n    if nodes:\n        return nodes\n\n    endpoint = str(os.environ.get('ACUNETIX_BASE_URL') or 'https://localhost:3443').strip().rstrip('/')\n    token = str((os.environ.get('ACUNETIX_API_KEY') or '').strip() or (os.environ.get('ACU_API_TOKEN') or '').strip())\n    fallback = normalize_selected_node({\n        'name': 'acu-default',\n        'endpoint': endpoint,\n        'api_key': token,\n    })\n    return [fallback] if fallback else []\n\n\ndef select_fallback_acu_node(nodes, node_policy, fallback_counts, sticky_key=None):\n    if not nodes:\n        return None\n    candidates = []\n    for node in nodes:\n        normalized = normalize_selected_node(node)\n        if not normalized:\n            continue\n        endpoint = normalized.get('endpoint')\n        if not endpoint:\n            continue\n        count = int(fallback_counts.get(endpoint, 0) or 0)\n        weight = parse_int(node.get('weight', normalized.get('weight', 1)), 1)\n        candidates.append({\n            'node': normalized,\n            'endpoint': endpoint,\n            'count': count,\n            'weight': weight,\n            'name': normalized.get('name') or '',\n        })\n    if not candidates:\n        return None\n\n    if node_policy == 'sticky':\n        sticky_token = str(sticky_key or '').strip()\n        if sticky_token:\n            chosen = sorted(candidates, key=lambda c: (c['name'], c['endpoint']))[sum(ord(ch) for ch in sticky_token) % len(candidates)]\n        else:\n            chosen = sorted(candidates, key=lambda c: (c['count'], c['name']))[0]\n    elif node_policy == 'weighted':\n        chosen = sorted(candidates, key=lambda c: (c['count'] / max(1, c['weight']), c['count'], c['name']))[0]\n    else:\n        chosen = sorted(candidates, key=lambda c: (c['count'], c['name']))[0]\n\n    fallback_counts[chosen['endpoint']] = int(fallback_counts.get(chosen['endpoint'], 0) or 0) + 1\n    return chosen['node']\n\n\ndef persist_pt_node_mapping(path, updates):\n    if not path or not isinstance(updates, dict) or not updates:\n        return\n    try:\n        payload = {'items': {}}\n        if os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                loaded = json.load(f)\n                if isinstance(loaded, dict) and isinstance(loaded.get('items'), dict):\n                    payload['items'] = loaded.get('items')\n        for pt_id, node in updates.items():\n            normalized = normalize_selected_node(node)\n            if not normalized:\n                continue\n            payload['items'][str(pt_id)] = {\n                'endpoint': normalized.get('endpoint'),\n                'api_key': normalized.get('api_key'),\n                'token': normalized.get('token'),\n                'name': normalized.get('name'),\n                'weight': normalized.get('weight', 1),\n                'source': 'primary_pt_state',\n            }\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(payload, f, ensure_ascii=False, indent=2)\n    except Exception:\n        pass\n\n\ndef parse_ts(value):\n    if not isinstance(value, str) or not value.strip():\n        return None\n    raw = value.strip()\n    if raw.endswith('Z'):\n        raw = raw[:-1] + '+00:00'\n    try:\n        dt = datetime.fromisoformat(raw)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt.astimezone(timezone.utc)\n    except Exception:\n        return None\n\n\nincoming = to_container(payload)\ntrigger_input = to_container(read_key(incoming, 'trigger', {}))\ntrigger_product_type_id = read_key(trigger_input, 'product_type_id')\ntrigger_domain = read_key(trigger_input, 'domain')\n\ntry:\n    trigger_product_type_id = int(trigger_product_type_id) if trigger_product_type_id is not None else None\nexcept Exception:\n    trigger_product_type_id = None\n\npt_window_size = parse_int(read_key(trigger_input, 'pt_window_size', None), parse_int(os.getenv('PT_WINDOW_SIZE'), 1))\nsubdomains_limit = parse_int(read_key(trigger_input, 'subdomains_concurrency', None), parse_int(os.getenv('SUBDOMAINS_CONCURRENCY'), 5))\nnmap_limit = parse_int(read_key(trigger_input, 'nmap_concurrency', None), parse_int(os.getenv('NMAP_CONCURRENCY'), 5))\nsubdomains_stale_minutes = parse_int(read_key(trigger_input, 'subdomains_running_timeout_minutes', None), parse_int(os.getenv('SUBDOMAINS_RUNNING_TIMEOUT_MINUTES'), 60))\nlock_ttl_minutes = parse_int(read_key(trigger_input, 'pt_lock_ttl_minutes', None), parse_int(os.getenv('PT_LOCK_TTL_MINUTES'), 20))\n\nretry_limits = {\n    'subdomains': parse_int(os.getenv('PT_RETRY_SUBDOMAINS_MAX'), 3),\n    'nmap': parse_int(os.getenv('PT_RETRY_NMAP_MAX'), 3),\n    'targets': parse_int(os.getenv('PT_RETRY_TARGETS_MAX'), 3),\n    'acu': parse_int(os.getenv('PT_RETRY_ACU_MAX'), 3),\n}\n\nrun_lock_owner = str(read_key(trigger_input, 'lock_owner', '') or '').strip()\nif not run_lock_owner:\n    run_lock_owner = f\"dojo-master:{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S%f')}\"\n\nproduct_types = to_list(read_key(incoming, 'product_types', []))\nproducts = to_list(read_key(incoming, 'products', []))\n\npt_list = []\nfor pt in product_types:\n    pt_id = read_key(pt, 'id')\n    try:\n        pt_id = int(pt_id)\n    except Exception:\n        continue\n    if trigger_product_type_id is not None and pt_id != trigger_product_type_id:\n        continue\n    pt_list.append(pt)\n\nnow = datetime.now(timezone.utc)\nlock_until_iso = (now + timedelta(minutes=lock_ttl_minutes)).isoformat()\nacu_dispatch_policy = {\n    'fairness': 'round_robin_by_pt',\n    'node_selection': normalize_node_policy(os.getenv('ACUNETIX_NODE_SELECTION_POLICY')),\n    'sticky_assignment': parse_bool(os.getenv('ACUNETIX_STICKY_ASSIGNMENT', 'true'), True),\n}\ntargets_fallback_policy = normalize_targets_fallback_policy(os.getenv('ACUNETIX_TARGETS_FALLBACK_POLICY'))\nsubdomains_running_count = 0\nsubdomains_running_jobs = 0\nrecovered_stale_subdomains = 0\n\npt_node_mapping = load_pt_node_mapping(MAPPING_DB_PATH)\nacunetix_nodes = load_acunetix_nodes_from_env()\nfallback_assignment_counts = {}\npt_node_reconcile_events = []\npt_node_reconcile_updates = {}\n\nprepared_pts = []\nfor pt in sorted(pt_list, key=lambda x: int(read_key(x, 'id', 0) or 0)):\n    pt_id = int(read_key(pt, 'id'))\n    original_description = read_key(pt, 'description', '')\n    state_obj = parse_state(original_description) or {}\n    mapped_node = normalize_selected_node(pt_node_mapping.get(str(pt_id)))\n    state = normalize_state(state_obj.get('state'))\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    subdomains_total = int(counters.get('subdomains_total', state_obj.get('subdomains_total', 0)) or 0)\n    subdomains_done = int(counters.get('subdomains_done', state_obj.get('subdomains_done', 0)) or 0)\n    subdomains_failed = int(counters.get('subdomains_failed', state_obj.get('subdomains_failed', 0)) or 0)\n    subdomains_running = int(counters.get('subdomains_running', state_obj.get('subdomains_running', 0)) or 0)\n\n    lock_owner = str(state_obj.get('lock_owner') or '')\n    lock_until_dt = parse_ts(state_obj.get('lock_until'))\n    lock_active = lock_until_dt is not None and lock_until_dt > now and lock_owner and lock_owner != run_lock_owner\n\n    if state == 'subdomains_running':\n        last_update_dt = parse_ts(state_obj.get('last_update'))\n        stale = False\n        if last_update_dt is None:\n            stale = True\n        else:\n            stale = now - last_update_dt > timedelta(minutes=subdomains_stale_minutes)\n\n        if stale:\n            recovered_stale_subdomains += 1\n            retry_count += 1\n            state = 'error'\n            counters['subdomains_running'] = 0\n            updated = {\n                'version': int(state_obj.get('version', 1) or 1),\n                'state': 'error',\n                'counters': counters,\n                'last_update': now.isoformat(),\n                'retry_count': retry_count,\n                'last_error': f'Recovered stale subdomains_running on restart (timeout {subdomains_stale_minutes}m).',\n                'last_stage': 'subdomains',\n                'lock_owner': None,\n                'lock_until': None,\n            }\n            prepared_pts.append({'pt': pt, 'state': state, 'state_obj': updated, 'original_description': original_description, 'lock_active': False, 'mapped_node': mapped_node})\n            continue\n\n        subdomains_running_count += 1\n\n    if state == 'subdomains_running':\n        subdomains_running_jobs += max(1, subdomains_running)\n    prepared_pts.append({'pt': pt, 'state': state, 'state_obj': state_obj, 'original_description': original_description, 'lock_active': lock_active, 'mapped_node': mapped_node})\n\nsubdomains_slots = max(0, subdomains_limit - subdomains_running_jobs)\n\neligible = []\nfor row in prepared_pts:\n    if row.get('lock_active'):\n        continue\n    if row['state'] not in ('new', 'error', 'subdomains_running', 'subdomains_done', 'nmap_done', 'targets_ready'):\n        continue\n    if row['state'] == 'error':\n        state_obj = row.get('state_obj') if isinstance(row.get('state_obj'), dict) else {}\n        stage = str(state_obj.get('last_stage') or 'subdomains')\n        limit = retry_limits.get(stage, 3)\n        retries = int(state_obj.get('retry_count', 0) or 0)\n        if retries >= limit:\n            continue\n    eligible.append(row)\n\nselected_rows = eligible[:pt_window_size]\nselected_ids = set(int(read_key(row['pt'], 'id')) for row in selected_rows)\n\nqueue_wf_a_subdomains_pt = []\nqueue_wf_b_nmap_product = []\nqueue_wf_c_targets_for_pt = []\nqueue_wf_d_pt_acunetixscan = []\nstate_updates = []\n\nfor row in prepared_pts:\n    if row.get('state_obj', {}).get('state') == 'error' and row['state'] == 'error' and 'Recovered stale subdomains_running' in str(row['state_obj'].get('last_error', '')):\n        pt_id = int(read_key(row['pt'], 'id'))\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(row['original_description'], row['state_obj']),\n            'state': 'error',\n        })\n\nfor row in selected_rows:\n    pt = row['pt']\n    pt_id = int(read_key(pt, 'id'))\n    pt_name = read_key(pt, 'name')\n    original_description = row['original_description']\n    state_obj = row['state_obj'] if isinstance(row['state_obj'], dict) else {}\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    current_state = row['state']\n\n    mapped_node = normalize_selected_node(row.get('mapped_node'))\n    state_selected_node = normalize_selected_node(state_obj.get('selected_acu_node'))\n    if state_selected_node is None:\n        state_selected_node = resolve_node_from_reference(state_obj.get('selected_acu_node'), acunetix_nodes)\n    if state_selected_node is None:\n        state_selected_node = resolve_node_from_reference({\n            'endpoint': state_obj.get('acunetix_endpoint'),\n            'name': state_obj.get('acunetix_node_name'),\n        }, acunetix_nodes)\n    if state_selected_node and mapped_node and not nodes_equal(state_selected_node, mapped_node):\n        pt_node_reconcile_events.append({\n            'stage': 'wf_dojo_master',\n            'pt_id': pt_id,\n            'message': 'PTâ†’Acunetix node mismatch detected; secondary cache synchronized from PT-state primary binding.',\n            'primary': {'endpoint': state_selected_node.get('endpoint'), 'name': state_selected_node.get('name')},\n            'secondary_before': {'endpoint': mapped_node.get('endpoint'), 'name': mapped_node.get('name')},\n        })\n        pt_node_reconcile_updates[str(pt_id)] = state_selected_node\n    resolved_acu_node = state_selected_node or mapped_node\n\n    next_state = None\n    queue_name = None\n    dispatched_subdomain_jobs = 0\n    if current_state in ('new', 'error', 'subdomains_running'):\n        subdomains_total = int(counters.get('subdomains_total', state_obj.get('subdomains_total', 0)) or 0)\n        subdomains_done = int(counters.get('subdomains_done', state_obj.get('subdomains_done', 0)) or 0)\n        subdomains_failed = int(counters.get('subdomains_failed', state_obj.get('subdomains_failed', 0)) or 0)\n        subdomains_running = int(counters.get('subdomains_running', state_obj.get('subdomains_running', 0)) or 0)\n\n        if subdomains_total <= 0:\n            subdomains_total = max(1, subdomains_done + subdomains_failed + subdomains_running + 1)\n\n        pending_subdomain_jobs = max(0, subdomains_total - subdomains_done - subdomains_failed - subdomains_running)\n        if subdomains_slots > 0 and pending_subdomain_jobs > 0:\n            dispatched_subdomain_jobs = min(subdomains_slots, pending_subdomain_jobs)\n            next_state = 'subdomains_running'\n            queue_name = 'a'\n            subdomains_slots -= dispatched_subdomain_jobs\n    elif current_state in ('subdomains_done', 'nmap_running'):\n        queue_name = 'b'\n    elif current_state == 'nmap_done':\n        next_state = 'targets_ready'\n        queue_name = 'c'\n    elif current_state == 'targets_ready':\n        next_state = 'acu_running'\n        queue_name = 'd'\n\n    if queue_name == 'a':\n        for job_idx in range(dispatched_subdomain_jobs):\n            queue_wf_a_subdomains_pt.append({\n                'pt_id': pt_id,\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain,\n                'stage': 'subdomains',\n                'subdomain_job_index': job_idx + 1,\n                'subdomain_job_batch_size': dispatched_subdomain_jobs,\n                'job_metadata': {\n                    'source_workflow': 'WF_Dojo_Master',\n                    'queue': 'wf_a_subdomains_pt',\n                    'transition': f\"{current_state}->subdomains_running\",\n                    'subdomain_job_granularity': 'pt_internal_job',\n                    'lock_owner': run_lock_owner,\n                },\n            })\n    elif queue_name == 'b':\n        jobs = []\n        for product in products:\n            try:\n                product_pt = int(read_key(product, 'prod_type'))\n                product_id = int(read_key(product, 'id'))\n            except Exception:\n                continue\n            if product_pt != pt_id:\n                continue\n            jobs.append({\n                'product_id': product_id,\n                'product_name': read_key(product, 'name'),\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain\n            })\n        jobs = sorted(jobs, key=lambda p: int(read_key(p, 'product_id', 0) or 0))\n\n        nmap_total = int(counters.get('nmap_total', 0) or 0)\n        if nmap_total <= 0:\n            nmap_total = len(jobs)\n        nmap_done = int(counters.get('nmap_done', 0) or 0)\n        nmap_failed = int(counters.get('nmap_failed', 0) or 0)\n\n        processed = max(0, min(nmap_total, nmap_done + nmap_failed))\n        pending_jobs = jobs[processed:]\n        batch = pending_jobs[:nmap_limit]\n\n        counters['nmap_total'] = nmap_total\n        counters['nmap_done'] = nmap_done\n        counters['nmap_failed'] = nmap_failed\n\n        for job in batch:\n            queue_wf_b_nmap_product.append({\n                **job,\n                'pt_id': pt_id,\n                'stage': 'nmap',\n                'job_metadata': {\n                    'source_workflow': 'WF_Dojo_Master',\n                    'queue': 'wf_b_nmap_product',\n                    'transition': f\"{current_state}->nmap_running\",\n                    'lock_owner': run_lock_owner,\n                },\n                'nmap_total': nmap_total,\n                'nmap_done_initial': nmap_done,\n                'nmap_failed_initial': nmap_failed,\n                'nmap_state_description': original_description,\n            })\n\n        if nmap_total == 0 or processed >= nmap_total:\n            next_state = 'nmap_done'\n        elif batch:\n            next_state = 'nmap_running'\n    elif queue_name == 'c':\n        selected_acu_node = resolved_acu_node\n        acunetix_endpoint = (selected_acu_node or {}).get('endpoint') or state_obj.get('acunetix_endpoint')\n        acunetix_api_key = (selected_acu_node or {}).get('api_key') or (selected_acu_node or {}).get('token')\n        acunetix_node_name = (selected_acu_node or {}).get('name') or state_obj.get('acunetix_node_name')\n\n        if selected_acu_node is None and acunetix_endpoint and acunetix_api_key:\n            selected_acu_node = normalize_selected_node({\n                'endpoint': acunetix_endpoint,\n                'api_key': acunetix_api_key,\n                'token': acunetix_api_key,\n                'name': acunetix_node_name or 'acu-selected',\n            })\n\n        if selected_acu_node is None:\n            selected_acu_node = select_fallback_acu_node(\n                acunetix_nodes,\n                targets_fallback_policy,\n                fallback_assignment_counts,\n                sticky_key=f\"pt:{pt_id}\",\n            )\n\n        if selected_acu_node:\n            acunetix_endpoint = selected_acu_node.get('endpoint') or acunetix_endpoint\n            acunetix_api_key = selected_acu_node.get('api_key') or selected_acu_node.get('token') or acunetix_api_key\n            acunetix_node_name = selected_acu_node.get('name') or acunetix_node_name\n            resolved_acu_node = selected_acu_node\n\n        queue_wf_c_targets_for_pt.append({\n            'pt_id': pt_id,\n            'product_type_id': pt_id,\n            'product_type_name': pt_name,\n            'domain': trigger_domain,\n            'stage': 'targets',\n            'selected_acu_node': selected_acu_node,\n            'acunetix_endpoint': acunetix_endpoint,\n            'acunetix_node_name': acunetix_node_name,\n            'job_metadata': {\n                'source_workflow': 'WF_Dojo_Master',\n                'queue': 'wf_c_targets_for_pt',\n                'transition': f\"{current_state}->targets_ready\",\n                'lock_owner': run_lock_owner,\n            },\n        })\n    elif queue_name == 'd':\n        queue_wf_d_pt_acunetixscan.append({\n            'pt_id': pt_id,\n            'product_type_id': pt_id,\n            'product_type_name': pt_name,\n            'domain': trigger_domain,\n            'stage': 'acu',\n            'selected_acu_node': resolved_acu_node,\n            'dispatch_policy': acu_dispatch_policy,\n            'job_metadata': {\n                'source_workflow': 'WF_Dojo_Master',\n                'queue': 'wf_d_pt_acunetixscan',\n                'transition': f\"{current_state}->acu_running\",\n                'lock_owner': run_lock_owner,\n            },\n        })\n\n    if next_state:\n        if next_state == 'subdomains_running':\n            counters['subdomains_total'] = int(counters.get('subdomains_total', 0) or 0)\n            counters['subdomains_done'] = int(counters.get('subdomains_done', 0) or 0)\n            counters['subdomains_failed'] = int(counters.get('subdomains_failed', 0) or 0)\n            counters['subdomains_running'] = int(counters.get('subdomains_running', 0) or 0) + max(0, dispatched_subdomain_jobs)\n            min_total = counters['subdomains_done'] + counters['subdomains_failed'] + counters['subdomains_running']\n            if counters['subdomains_total'] < min_total:\n                counters['subdomains_total'] = min_total\n\n        stage_key = {\n            'subdomains_running': 'subdomains_runs',\n            'nmap_running': 'nmap_runs',\n            'targets_ready': 'targets_runs',\n            'acu_running': 'acu_runs',\n        }.get(next_state)\n        if stage_key:\n            stage_increment = max(1, dispatched_subdomain_jobs) if next_state == 'subdomains_running' else 1\n            counters[stage_key] = int(counters.get(stage_key, 0) or 0) + stage_increment\n\n        last_stage = {\n            'subdomains_running': 'subdomains',\n            'nmap_running': 'nmap',\n            'targets_ready': 'targets',\n            'acu_running': 'acu',\n        }.get(next_state, state_obj.get('last_stage'))\n\n        updated = {\n            'version': int(state_obj.get('version', 1) or 1),\n            'state': next_state,\n            'counters': counters,\n            'subdomains_total': int(counters.get('subdomains_total', 0) or 0),\n            'subdomains_done': int(counters.get('subdomains_done', 0) or 0),\n            'subdomains_failed': int(counters.get('subdomains_failed', 0) or 0),\n            'subdomains_running': int(counters.get('subdomains_running', 0) or 0),\n            'last_update': now.isoformat(),\n            'retry_count': retry_count,\n            'last_error': None if next_state == 'subdomains_running' else state_obj.get('last_error'),\n            'last_stage': last_stage,\n            'lock_owner': run_lock_owner if next_state in {'subdomains_running', 'nmap_running', 'targets_ready', 'acu_running'} else None,\n            'lock_until': lock_until_iso if next_state in {'subdomains_running', 'nmap_running', 'targets_ready', 'acu_running'} else None,\n        }\n        if next_state in {'targets_ready', 'acu_running'}:\n            bound_node = resolved_acu_node\n            if bound_node:\n                updated['selected_acu_node'] = {\n                    'endpoint': bound_node.get('endpoint'),\n                    'name': bound_node.get('name'),\n                }\n                updated['acunetix_endpoint'] = bound_node.get('endpoint')\n                updated['acunetix_node_name'] = bound_node.get('name')\n        if next_state == 'acu_running':\n            updated['acu_dispatch_policy'] = acu_dispatch_policy\n\n        updated.pop('acunetix_api_key', None)\n        updated.pop('acunetix_token', None)\n        selected_ref = updated.get('selected_acu_node')\n        if isinstance(selected_ref, dict):\n            updated['selected_acu_node'] = {\n                'endpoint': selected_ref.get('endpoint'),\n                'name': selected_ref.get('name'),\n            }\n\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(original_description, updated),\n            'state': next_state,\n        })\n\n\ndef build_targets_stage_contract_smoke(queue_rows):\n    invalid = []\n    for idx, row in enumerate(to_list(queue_rows)):\n        if not isinstance(row, dict):\n            invalid.append({'index': idx, 'missing_fields': list(TARGETS_STAGE_CONTRACT_FIELDS)})\n            continue\n        missing = [field for field in TARGETS_STAGE_CONTRACT_FIELDS if field not in row]\n        if missing:\n            invalid.append({'index': idx, 'missing_fields': missing})\n    return {\n        'stage': 'targets',\n        'required_fields': list(TARGETS_STAGE_CONTRACT_FIELDS),\n        'queue_size': len(to_list(queue_rows)),\n        'ok': len(invalid) == 0,\n        'invalid_rows': invalid,\n    }\n\nmapping_upserts = persist_pt_node_mapping(MAPPING_DB_PATH, pt_node_reconcile_updates)\n\nreturn [{\n    'json': {\n        'run_subdomains': len(queue_wf_a_subdomains_pt) > 0,\n        'run_nmap': len(queue_wf_b_nmap_product) > 0,\n        'run_targets': len(queue_wf_c_targets_for_pt) > 0,\n        'run_acunetix': len(queue_wf_d_pt_acunetixscan) > 0,\n        'queue_wf_a_subdomains_pt': queue_wf_a_subdomains_pt,\n        'queue_wf_b_nmap_product': queue_wf_b_nmap_product,\n        'queue_wf_c_targets_for_pt': queue_wf_c_targets_for_pt,\n        'queue_wf_d_pt_acunetixscan': queue_wf_d_pt_acunetixscan,\n        'state_updates': state_updates,\n        'trigger_product_type_id': trigger_product_type_id,\n        'trigger_domain': trigger_domain,\n        'pt_window_size': pt_window_size,\n        'selected_pt_ids': sorted(list(selected_ids)),\n        'subdomains_concurrency': subdomains_limit,\n        'subdomains_running_now': subdomains_running_count,\n        'subdomains_running_jobs': subdomains_running_jobs,\n        'subdomains_slots_available': max(0, subdomains_limit - subdomains_running_jobs),\n        'subdomains_stale_recovered': recovered_stale_subdomains,\n        'subdomains_running_timeout_minutes': subdomains_stale_minutes,\n        'nmap_concurrency': nmap_limit,\n        'pt_lock_ttl_minutes': lock_ttl_minutes,\n        'lock_owner': run_lock_owner,\n        'retry_limits': retry_limits,\n        'stage_barriers': {\n            'subdomains_done_after_all_jobs': True,\n            'nmap_done_after_all_jobs': True,\n        },\n        'acu_dispatch_policy': acu_dispatch_policy,\n        'targets_fallback_policy': targets_fallback_policy,\n        'targets_stage_contract_smoke': build_targets_stage_contract_smoke(queue_wf_c_targets_for_pt),\n        'pt_node_reconcile_events': pt_node_reconcile_events,\n        'target_mapping_store': MAPPING_DB_PATH,\n        'mapping_upserts': mapping_upserts,\n        'plan_input_source': plan_input_source,\n        'planner_version': PLANNER_VERSION,\n    }\n}]"
      },
      "id": "m-plan",
      "name": "Plan Builder Core (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -950,
        100
      ]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_subdomains}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-subdomains",
      "name": "Need WF_A_Subdomains_PT?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -760,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_A_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-a",
      "name": "Run WF_A_Subdomains_PT",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        -460,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_nmap}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-nmap",
      "name": "Need WF_B_Nmap_Product?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -340,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_B_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-b",
      "name": "Run WF_B_Nmap_Product",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        -40,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_targets}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-targets",
      "name": "Need WF_C_Targets_For_PT?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        60,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_C_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-c",
      "name": "Run WF_C_Targets_For_PT",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        360,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_acunetix}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-acu",
      "name": "Need WF_D_PT_AcunetixScan?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        460,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_D_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-d",
      "name": "Run WF_D_PT_AcunetixScan",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        670,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "operation": "splitOut",
        "fieldToSplitOut": "queue_wf_a_subdomains_pt",
        "options": {}
      },
      "id": "m-expand-a",
      "name": "Expand WF_A Candidates (Items)",
      "type": "n8n-nodes-base.itemLists",
      "typeVersion": 3,
      "position": [
        -660,
        20
      ]
    },
    {
      "parameters": {
        "operation": "splitOut",
        "fieldToSplitOut": "queue_wf_b_nmap_product",
        "options": {}
      },
      "id": "m-expand-b",
      "name": "Expand WF_B Candidates (Items)",
      "type": "n8n-nodes-base.itemLists",
      "typeVersion": 3,
      "position": [
        -240,
        20
      ]
    },
    {
      "parameters": {
        "operation": "splitOut",
        "fieldToSplitOut": "queue_wf_c_targets_for_pt",
        "options": {}
      },
      "id": "m-expand-c",
      "name": "Expand WF_C Candidates (Items)",
      "type": "n8n-nodes-base.itemLists",
      "typeVersion": 3,
      "position": [
        160,
        20
      ]
    },
    {
      "parameters": {
        "operation": "splitOut",
        "fieldToSplitOut": "queue_wf_d_pt_acunetixscan",
        "options": {}
      },
      "id": "m-expand-d",
      "name": "Expand WF_D Candidates (Items)",
      "type": "n8n-nodes-base.itemLists",
      "typeVersion": 3,
      "position": [
        560,
        20
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "trigger",
              "value": "={{$json}}"
            }
          ]
        },
        "options": {}
      },
      "id": "m-prepare-trigger",
      "name": "Prepare Trigger Input",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        -1170,
        -80
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'product_types': aggregated}}]"
      },
      "id": "m-prepare-pt",
      "name": "Prepare Product Types (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -960,
        -80
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'products': aggregated, 'plan_input_source': 'normal'}}]"
      },
      "id": "m-prepare-products",
      "name": "Prepare Products (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -740,
        280
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-trigger-pt",
      "name": "Merge Trigger + Product Types",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        -740,
        -80
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-build-input",
      "name": "Merge Build Plan Input",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        -520,
        100
      ]
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-product-types-after-a",
      "name": "Get Product Types After WF_A",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -300,
        -260
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/products/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-products-after-a",
      "name": "Get Products After WF_A",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -300,
        -120
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'product_types': aggregated}}]"
      },
      "id": "m-prepare-pt-after-a",
      "name": "Prepare Product Types After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -260
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'products': aggregated, 'plan_input_source': 'rebuild'}}]"
      },
      "id": "m-prepare-products-after-a",
      "name": "Prepare Products After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -120
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-trigger-pt-after-a",
      "name": "Merge Trigger + Product Types After WF_A",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        150,
        -220
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-rebuild-input",
      "name": "Merge Rebuild Plan Input",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        340,
        -140
      ]
    },
    {
      "parameters": {
        "operation": "splitOut",
        "fieldToSplitOut": "state_updates",
        "options": {}
      },
      "id": "m-expand-state-updates",
      "name": "Expand PT State Updates",
      "type": "n8n-nodes-base.itemLists",
      "typeVersion": 3,
      "position": [
        -520,
        220
      ]
    },
    {
      "parameters": {
        "method": "PATCH",
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/' + $json.product_type_id + '/' }}",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { description: $json.description } }}",
        "options": {}
      },
      "id": "m-patch-pt-state",
      "name": "Patch PT State Description",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -260,
        220
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      },
      "retryOnFail": true,
      "maxTries": 3,
      "waitBetweenTries": 2000,
      "continueOnFail": true
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport re\nimport os\nfrom datetime import datetime, timezone\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\n\ndef read_key(container, key, default=None):\n    try:\n        val = container.get(key)\n        return default if val is None else val\n    except Exception:\n        try:\n            val = container[key]\n            return default if val is None else val\n        except Exception:\n            return default\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return {}\n    m = re.search(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", description, re.DOTALL)\n    if not m:\n        return {}\n    try:\n        state = json.loads(m.group(1).strip())\n        return state if isinstance(state, dict) else {}\n    except Exception:\n        return {}\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\nitems = _input.all()\nif not items:\n    return []\n\nmax_retries = int(os.getenv('PT_RETRY_NMAP_MAX', '3') or 3)\nnow = datetime.now(timezone.utc).isoformat()\nby_pt = {}\nfor item in items:\n    row = item.json if isinstance(item.json, dict) else {}\n    try:\n        pt_id = int(read_key(row, 'product_type_id'))\n    except Exception:\n        continue\n\n    agg = by_pt.get(pt_id)\n    if agg is None:\n        agg = {\n            'description': read_key(row, 'nmap_state_description', ''),\n            'nmap_total': int(read_key(row, 'nmap_total', 0) or 0),\n            'nmap_done': int(read_key(row, 'nmap_done_initial', 0) or 0),\n            'nmap_failed': int(read_key(row, 'nmap_failed_initial', 0) or 0),\n            'done_inc': 0,\n            'failed_inc': 0,\n            'errors': [],\n        }\n        by_pt[pt_id] = agg\n\n    status = str(read_key(row, 'workflow_status', 'ok') or 'ok')\n    if status == 'ok':\n        agg['done_inc'] += 1\n    else:\n        agg['failed_inc'] += 1\n        err = str(read_key(row, 'error_message', '') or '').strip()\n        if err:\n            agg['errors'].append(err)\n\nout = []\nfor pt_id, agg in by_pt.items():\n    state_obj = parse_state(agg['description'])\n    counters = read_key(state_obj, 'counters', {})\n    if not isinstance(counters, dict):\n        counters = {}\n\n    nmap_total = agg['nmap_total'] if agg['nmap_total'] > 0 else int(counters.get('nmap_total', 0) or 0)\n    nmap_done = int(agg['nmap_done']) + int(agg['done_inc'])\n    nmap_failed = int(agg['nmap_failed']) + int(agg['failed_inc'])\n    processed_total = nmap_done + nmap_failed\n\n    counters['nmap_total'] = nmap_total\n    counters['nmap_done'] = nmap_done\n    counters['nmap_failed'] = nmap_failed\n\n    retry_count = int(read_key(state_obj, 'retry_count', 0) or 0)\n    last_error = read_key(state_obj, 'last_error')\n\n    if agg['failed_inc'] > 0:\n        retry_count += 1\n        diag = '; '.join([e for e in agg['errors'] if e][:2])\n        if not diag:\n            diag = f'nmap stage reported {agg[\"failed_inc\"]} failed job(s)'\n        last_error = f'NMAP error: {diag}'\n\n    all_nmap_jobs_processed = nmap_total == 0 or processed_total >= nmap_total\n    has_nmap_failures = nmap_failed > 0\n    has_pending_nmap_jobs = nmap_total > 0 and processed_total < nmap_total\n    retries_exhausted = retry_count >= max_retries\n    failure_mode = 'none'\n\n    if has_nmap_failures:\n        if retries_exhausted and not has_pending_nmap_jobs:\n            next_state = 'error'\n            failure_mode = 'terminal'\n        else:\n            next_state = 'nmap_running'\n            failure_mode = 'partial_retryable'\n    else:\n        next_state = 'nmap_done' if all_nmap_jobs_processed else 'nmap_running'\n        if next_state == 'nmap_done':\n            last_error = None\n\n    updated_state = {\n        'version': int(read_key(state_obj, 'version', 1) or 1),\n        'state': next_state,\n        'counters': counters,\n        'last_update': now,\n        'retry_count': retry_count,\n        'failure_mode': failure_mode,\n        'last_error': last_error,\n        'last_stage': 'nmap',\n        'lock_owner': None,\n        'lock_until': None,\n    }\n\n    out.append({'json': {\n        'product_type_id': pt_id,\n        'state_before': str(read_key(state_obj, 'state', 'new') or 'new'),\n        'state_after': next_state,\n        'state': next_state,\n        'description': build_description(agg['description'], updated_state),\n        'workflow_status': 'ok' if next_state != 'error' else 'error',\n        'state_patch_status': 'state_patch_pending',\n        'state_patch_context': 'wf_b_summary',\n        'report_node': 'wf_b_summary',\n    }})\n\nreturn out\n"
      },
      "name": "Summarize WF_B Results (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1904,
        432
      ],
      "id": "m-summarize-b"
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.state_patch_context || ''}}",
              "operation": "equal",
              "value2": "wf_b_summary"
            }
          ]
        }
      },
      "id": "m-if-patch-from-wfb",
      "name": "Patch From WF_B Summary?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -60,
        220
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "pt_id",
              "value": "={{$json.pt_id ?? $json.product_type_id}}"
            },
            {
              "name": "state_before",
              "value": "={{$json.state_before ?? \"\"}}"
            },
            {
              "name": "state_after",
              "value": "={{$json.state_after ?? $json.state ?? \"\"}}"
            },
            {
              "name": "state_patch_status",
              "value": "={{$json.error ? \"fail\" : \"success\"}}"
            },
            {
              "name": "status_code",
              "value": "={{$json.statusCode ?? \"n/a\"}}"
            },
            {
              "name": "state_patch_error",
              "value": "={{$json.error ? JSON.stringify($json.error) : \"\"}}"
            }
          ]
        },
        "options": {}
      },
      "id": "m-finalize-state-patch-report",
      "name": "Finalize State Patch Report (Set)",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        120,
        220
      ]
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.state_patch_status}}",
              "operation": "equal",
              "value2": "success"
            }
          ]
        }
      },
      "id": "m-if-state-patch-ok",
      "name": "State Patch OK?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        300,
        220
      ]
    },
    {
      "parameters": {
        "message": "={{ 'PT-state PATCH failed. pt_id=' + String($json.pt_id || 'unknown') + '; state_before=' + String($json.state_before || 'unknown') + '; state_after=' + String($json.state_after || 'unknown') + '; status=' + String($json.status_code || 'n/a') + '; error=' + String($json.state_patch_error || 'n/a') }}"
      },
      "id": "m-stop-patch-error",
      "name": "Stop on PT State PATCH Error",
      "type": "n8n-nodes-base.stopAndError",
      "typeVersion": 1,
      "position": [
        500,
        300
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "pt_id",
              "value": "={{$json.pt_id ?? $json.product_type_id}}"
            },
            {
              "name": "product_type_id",
              "value": "={{$json.product_type_id}}"
            },
            {
              "name": "product_type_name",
              "value": "={{$json.product_type_name}}"
            },
            {
              "name": "domain",
              "value": "={{$json.domain}}"
            },
            {
              "name": "stage",
              "value": "={{$json.stage || 'subdomains'}}"
            },
            {
              "name": "job_metadata",
              "value": "={{$json.job_metadata ?? {}}}"
            }
          ]
        },
        "options": {}
      },
      "id": "set-m-expand-a",
      "name": "Map WF_A Candidate",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        -660,
        20
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "pt_id",
              "value": "={{$json.pt_id ?? $json.product_type_id}}"
            },
            {
              "name": "product_id",
              "value": "={{$json.product_id}}"
            },
            {
              "name": "product_name",
              "value": "={{$json.product_name}}"
            },
            {
              "name": "product_type_id",
              "value": "={{$json.product_type_id}}"
            },
            {
              "name": "product_type_name",
              "value": "={{$json.product_type_name}}"
            },
            {
              "name": "domain",
              "value": "={{$json.domain}}"
            },
            {
              "name": "stage",
              "value": "={{$json.stage || 'nmap'}}"
            },
            {
              "name": "job_metadata",
              "value": "={{$json.job_metadata ?? {}}}"
            },
            {
              "name": "nmap_total",
              "value": "={{$json.nmap_total}}"
            },
            {
              "name": "nmap_done_initial",
              "value": "={{$json.nmap_done_initial}}"
            },
            {
              "name": "nmap_failed_initial",
              "value": "={{$json.nmap_failed_initial}}"
            },
            {
              "name": "nmap_state_description",
              "value": "={{$json.nmap_state_description}}"
            }
          ]
        },
        "options": {}
      },
      "id": "set-m-expand-b",
      "name": "Map WF_B Candidate",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        -240,
        20
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "pt_id",
              "value": "={{$json.pt_id ?? $json.product_type_id}}"
            },
            {
              "name": "product_type_id",
              "value": "={{$json.product_type_id}}"
            },
            {
              "name": "product_type_name",
              "value": "={{$json.product_type_name}}"
            },
            {
              "name": "domain",
              "value": "={{$json.domain}}"
            },
            {
              "name": "stage",
              "value": "={{$json.stage || 'targets'}}"
            },
            {
              "name": "job_metadata",
              "value": "={{$json.job_metadata ?? {}}}"
            },
            {
              "name": "selected_acu_node",
              "value": "={{$json.selected_acu_node}}"
            },
            {
              "name": "acunetix_node_name",
              "value": "={{$json.acunetix_node_name}}"
            },
            {
              "name": "acunetix_endpoint",
              "value": "={{$json.acunetix_endpoint}}"
            }
          ]
        },
        "options": {}
      },
      "id": "set-m-expand-c",
      "name": "Map WF_C Candidate",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        160,
        20
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "pt_id",
              "value": "={{$json.pt_id ?? $json.product_type_id}}"
            },
            {
              "name": "product_type_id",
              "value": "={{$json.product_type_id}}"
            },
            {
              "name": "product_type_name",
              "value": "={{$json.product_type_name}}"
            },
            {
              "name": "domain",
              "value": "={{$json.domain}}"
            },
            {
              "name": "stage",
              "value": "={{$json.stage || 'acu'}}"
            },
            {
              "name": "job_metadata",
              "value": "={{$json.job_metadata ?? {}}}"
            },
            {
              "name": "dispatch_policy",
              "value": "={{$json.dispatch_policy ?? {}}}"
            },
            {
              "name": "selected_acu_node",
              "value": "={{$json.selected_acu_node}}"
            }
          ]
        },
        "options": {}
      },
      "id": "set-m-expand-d",
      "name": "Map WF_D Candidate",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        560,
        20
      ]
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.product_type_id}}",
              "operation": "notEmpty"
            },
            {
              "value1": "={{$json.description}}",
              "operation": "notEmpty"
            }
          ]
        }
      },
      "id": "if-pt-state",
      "name": "PT State Update Complete?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -340,
        220
      ]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "json": [
            {
              "name": "product_type_id",
              "value": "={{$json.product_type_id}}"
            },
            {
              "name": "description",
              "value": "={{$json.description}}"
            }
          ]
        },
        "options": {}
      },
      "id": "set-pt-state",
      "name": "Map PT State Update",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        -170,
        220
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport os\nimport re\n\nitem = _input.first()\nrow = item.json if item else {}\n\nrequired = ['queue_wf_a_subdomains_pt','queue_wf_b_nmap_product','queue_wf_c_targets_for_pt','queue_wf_d_pt_acunetixscan','state_updates','selected_pt_ids','planner_version']\n\ndef kind(v):\n    if isinstance(v, list):\n        return 'list'\n    if isinstance(v, dict):\n        return 'dict'\n    return type(v).__name__\n\ndef signature(obj):\n    return {k: kind(obj.get(k)) for k in required}\n\nsource = str(row.get('plan_input_source') or 'normal').strip().lower()\nlock_owner = re.sub(r'[^a-zA-Z0-9_.-]+','_', str(row.get('lock_owner') or 'default'))\nstate_file = f\"/tmp/wf_dojo_plan_shape_{lock_owner}.json\"\nsig = signature(row if isinstance(row, dict) else {})\nmissing = [k for k in required if k not in row]\nresult = {\n    'source': source,\n    'required_fields': required,\n    'signature': sig,\n    'missing_fields': missing,\n    'ok': len(missing) == 0,\n}\n\nif source == 'normal':\n    with open(state_file,'w',encoding='utf-8') as f:\n        json.dump({'signature': sig, 'required': required}, f, ensure_ascii=False, sort_keys=True)\n    result['compare_status'] = 'baseline_saved'\nelif source == 'rebuild':\n    if os.path.exists(state_file):\n        with open(state_file,'r',encoding='utf-8') as f:\n            baseline = json.load(f)\n        baseline_sig = baseline.get('signature', {}) if isinstance(baseline, dict) else {}\n        diff = [k for k in required if sig.get(k) != baseline_sig.get(k)]\n        result['matches_normal_path'] = len(diff) == 0 and len(missing) == 0\n        result['planner_version_match'] = sig.get('planner_version') == baseline_sig.get('planner_version')\n        result['diff_fields'] = diff\n        result['compare_status'] = 'compared_with_normal'\n    else:\n        result['matches_normal_path'] = False\n        result['planner_version_match'] = False\n        result['diff_fields'] = required\n        result['compare_status'] = 'normal_baseline_missing'\n\nrow['plan_output_structure_check'] = result\nreturn [{'json': row}]"
      },
      "id": "m-compare-plan-structure",
      "name": "Compare Plan Output Structure (Normal vs Rebuild)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -780,
        -40
      ]
    }
  ],
  "connections": {
    "Trigger": {
      "main": [
        [
          {
            "node": "Get Product Types",
            "type": "main",
            "index": 0
          },
          {
            "node": "Prepare Trigger Input",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Products",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Products": {
      "main": [
        [
          {
            "node": "Prepare Products (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_A_Subdomains_PT?": {
      "main": [
        [
          {
            "node": "Expand WF_A Candidates (Items)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_B_Nmap_Product?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_A_Subdomains_PT": {
      "main": [
        [
          {
            "node": "Get Product Types After WF_A",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Products After WF_A",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_B_Nmap_Product?": {
      "main": [
        [
          {
            "node": "Expand WF_B Candidates (Items)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_C_Targets_For_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_B_Nmap_Product": {
      "main": [
        [
          {
            "node": "Summarize WF_B Results (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_C_Targets_For_PT?": {
      "main": [
        [
          {
            "node": "Expand WF_C Candidates (Items)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_D_PT_AcunetixScan?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_C_Targets_For_PT": {
      "main": [
        [
          {
            "node": "Need WF_D_PT_AcunetixScan?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_D_PT_AcunetixScan?": {
      "main": [
        [
          {
            "node": "Expand WF_D Candidates (Items)",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "Get Product Types": {
      "main": [
        [
          {
            "node": "Prepare Product Types (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Product Types (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare Products (Python)": {
      "main": [
        [
          {
            "node": "Merge Build Plan Input",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Trigger + Product Types": {
      "main": [
        [
          {
            "node": "Merge Build Plan Input",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge Trigger + Product Types After WF_A",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Build Plan Input": {
      "main": [
        [
          {
            "node": "Plan Builder Core (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Product Types After WF_A": {
      "main": [
        [
          {
            "node": "Prepare Product Types After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Products After WF_A": {
      "main": [
        [
          {
            "node": "Prepare Products After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Product Types After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types After WF_A",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare Products After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Merge Rebuild Plan Input",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Trigger + Product Types After WF_A": {
      "main": [
        [
          {
            "node": "Merge Rebuild Plan Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Rebuild Plan Input": {
      "main": [
        [
          {
            "node": "Plan Builder Core (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Summarize WF_B Results (Python)": {
      "main": [
        [
          {
            "node": "Patch PT State Description",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Patch PT State Description": {
      "main": [
        [
          {
            "node": "Patch From WF_B Summary?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Patch From WF_B Summary?": {
      "main": [
        [
          {
            "node": "Finalize State Patch Report (Set)",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "State Patch OK?": {
      "main": [
        [
          {
            "node": "Need WF_C_Targets_For_PT?",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Stop on PT State PATCH Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_A Candidates (Items)": {
      "main": [
        [
          {
            "node": "Map WF_A Candidate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Map WF_A Candidate": {
      "main": [
        [
          {
            "node": "Run WF_A_Subdomains_PT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_B Candidates (Items)": {
      "main": [
        [
          {
            "node": "Map WF_B Candidate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Map WF_B Candidate": {
      "main": [
        [
          {
            "node": "Run WF_B_Nmap_Product",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_C Candidates (Items)": {
      "main": [
        [
          {
            "node": "Map WF_C Candidate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Map WF_C Candidate": {
      "main": [
        [
          {
            "node": "Run WF_C_Targets_For_PT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_D Candidates (Items)": {
      "main": [
        [
          {
            "node": "Map WF_D Candidate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Map WF_D Candidate": {
      "main": [
        [
          {
            "node": "Run WF_D_PT_AcunetixScan",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Trigger Input": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand PT State Updates": {
      "main": [
        [
          {
            "node": "PT State Update Complete?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "PT State Update Complete?": {
      "main": [
        [
          {
            "node": "Map PT State Update",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "Map PT State Update": {
      "main": [
        [
          {
            "node": "Patch PT State Description",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compare Plan Output Structure (Normal vs Rebuild)": {
      "main": [
        [
          {
            "node": "Expand PT State Updates",
            "type": "main",
            "index": 0
          },
          {
            "node": "Need WF_A_Subdomains_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Finalize State Patch Report (Set)": {
      "main": [
        [
          {
            "node": "State Patch OK?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Plan Builder Core (Python)": {
      "main": [
        [
          {
            "node": "Compare Plan Output Structure (Normal vs Rebuild)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "tags": []
}