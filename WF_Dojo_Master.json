{
  "name": "WF_Dojo_Master",
  "nodes": [
    {
      "parameters": {},
      "id": "m-trigger",
      "name": "Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        -1400,
        100
      ]
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-product-types",
      "name": "Get Product Types",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -1360,
        100
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/products/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-products",
      "name": "Get Products",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -1180,
        100
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport os\nimport re\nfrom datetime import datetime, timezone, timedelta\n\nPT_NODE_MAPPING_FILE = (os.environ.get('ACUNETIX_PT_NODE_MAPPING_FILE') or '/tmp/acunetix_pt_node_mapping.json').strip()\n\nfirst_item = _input.first()\npayload = first_item.json if first_item else None\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\nVALID_STATES = {\n    'new', 'subdomains_running', 'subdomains_done',\n    'nmap_running', 'nmap_done', 'targets_ready',\n    'acu_running', 'done', 'error'\n}\n\nTARGETS_STAGE_CONTRACT_FIELDS = [\n    'pt_id', 'product_type_id', 'product_type_name', 'domain', 'stage',\n    'selected_acu_node', 'acunetix_endpoint', 'acunetix_api_key', 'acunetix_token', 'acunetix_node_name',\n    'job_metadata',\n]\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    return value\n\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return None\n    pattern = re.compile(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", re.DOTALL)\n    m = pattern.search(description)\n    if not m:\n        return None\n    try:\n        data = json.loads(m.group(1).strip())\n        if not isinstance(data, dict):\n            return None\n        return data\n    except Exception:\n        return None\n\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\n\ndef normalize_state(raw_state):\n    state = str(raw_state or 'new')\n    return state if state in VALID_STATES else 'new'\n\n\ndef parse_int(value, default):\n    try:\n        val = int(value)\n        return val if val > 0 else default\n    except Exception:\n        return default\n\n\ndef parse_bool(value, default=False):\n    if value is None:\n        return default\n    if isinstance(value, bool):\n        return value\n    raw = str(value).strip().lower()\n    if raw in {'1', 'true', 'yes', 'y', 'on'}:\n        return True\n    if raw in {'0', 'false', 'no', 'n', 'off'}:\n        return False\n    return default\n\n\ndef normalize_node_policy(value):\n    raw = str(value or '').strip().lower()\n    return raw if raw in {'least_loaded', 'weighted'} else 'least_loaded'\n\n\n\ndef load_pt_node_mapping(path):\n    if not path:\n        return {}\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            payload = json.load(f)\n        items = payload.get('items', {}) if isinstance(payload, dict) else {}\n        return items if isinstance(items, dict) else {}\n    except Exception:\n        return {}\n\n\ndef normalize_selected_node(raw):\n    if not isinstance(raw, dict):\n        return None\n    endpoint = str(raw.get('endpoint') or '').strip().rstrip('/')\n    token = str(raw.get('api_key') or raw.get('token') or '').strip()\n    if not endpoint or not token:\n        return None\n    return {\n        'endpoint': endpoint,\n        'api_key': token,\n        'token': token,\n        'name': str(raw.get('name') or 'acu-selected').strip() or 'acu-selected',\n        'weight': parse_int(raw.get('weight', 1), 1),\n    }\n\n\ndef normalize_node_reference(raw):\n    if not isinstance(raw, dict):\n        return None\n    endpoint = str(raw.get('endpoint') or '').strip().rstrip('/')\n    name = str(raw.get('name') or '').strip()\n    if not endpoint and not name:\n        return None\n    return {\n        'endpoint': endpoint,\n        'name': name,\n    }\n\n\ndef resolve_node_from_reference(raw, nodes):\n    ref = normalize_node_reference(raw)\n    if not ref:\n        return None\n    endpoint = ref.get('endpoint')\n    name = ref.get('name')\n    for node in nodes:\n        if endpoint and str(node.get('endpoint') or '').strip().rstrip('/') == endpoint:\n            return normalize_selected_node(node)\n    for node in nodes:\n        if name and str(node.get('name') or '').strip() == name:\n            return normalize_selected_node(node)\n    return None\n\n\ndef load_acunetix_nodes_from_env():\n    nodes = []\n    raw = (os.environ.get('ACUNETIX_INSTANCES_JSON') or '').strip()\n    if raw:\n        try:\n            arr = json.loads(raw)\n            if isinstance(arr, list):\n                for idx, item in enumerate(arr):\n                    node = normalize_selected_node(item)\n                    if node:\n                        if not str(node.get('name') or '').strip():\n                            node['name'] = f'acu-{idx + 1}'\n                        nodes.append(node)\n        except Exception:\n            nodes = []\n\n    if nodes:\n        return nodes\n\n    endpoint = str(os.environ.get('ACUNETIX_BASE_URL') or 'https://localhost:3443').strip().rstrip('/')\n    token = str((os.environ.get('ACUNETIX_API_KEY') or '').strip() or (os.environ.get('ACU_API_TOKEN') or '').strip())\n    fallback = normalize_selected_node({\n        'name': 'acu-default',\n        'endpoint': endpoint,\n        'api_key': token,\n    })\n    return [fallback] if fallback else []\n\n\ndef select_fallback_acu_node(nodes, node_policy, fallback_counts):\n    if not nodes:\n        return None\n    candidates = []\n    for node in nodes:\n        normalized = normalize_selected_node(node)\n        if not normalized:\n            continue\n        endpoint = normalized.get('endpoint')\n        if not endpoint:\n            continue\n        count = int(fallback_counts.get(endpoint, 0) or 0)\n        weight = parse_int(node.get('weight', normalized.get('weight', 1)), 1)\n        candidates.append({\n            'node': normalized,\n            'endpoint': endpoint,\n            'count': count,\n            'weight': weight,\n            'name': normalized.get('name') or '',\n        })\n    if not candidates:\n        return None\n\n    if node_policy == 'weighted':\n        chosen = sorted(candidates, key=lambda c: (c['count'] / max(1, c['weight']), c['count'], c['name']))[0]\n    else:\n        chosen = sorted(candidates, key=lambda c: (c['count'], c['name']))[0]\n\n    fallback_counts[chosen['endpoint']] = int(fallback_counts.get(chosen['endpoint'], 0) or 0) + 1\n    return chosen['node']\n\n\ndef parse_ts(value):\n    if not isinstance(value, str) or not value.strip():\n        return None\n    raw = value.strip()\n    if raw.endswith('Z'):\n        raw = raw[:-1] + '+00:00'\n    try:\n        dt = datetime.fromisoformat(raw)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt.astimezone(timezone.utc)\n    except Exception:\n        return None\n\n\nincoming = to_container(payload)\ntrigger_input = to_container(read_key(incoming, 'trigger', {}))\ntrigger_product_type_id = read_key(trigger_input, 'product_type_id')\ntrigger_domain = read_key(trigger_input, 'domain')\n\ntry:\n    trigger_product_type_id = int(trigger_product_type_id) if trigger_product_type_id is not None else None\nexcept Exception:\n    trigger_product_type_id = None\n\npt_window_size = parse_int(read_key(trigger_input, 'pt_window_size', None), parse_int(os.getenv('PT_WINDOW_SIZE'), 1))\nsubdomains_limit = parse_int(read_key(trigger_input, 'subdomains_concurrency', None), parse_int(os.getenv('SUBDOMAINS_CONCURRENCY'), 5))\nnmap_limit = parse_int(read_key(trigger_input, 'nmap_concurrency', None), parse_int(os.getenv('NMAP_CONCURRENCY'), 5))\nsubdomains_stale_minutes = parse_int(read_key(trigger_input, 'subdomains_running_timeout_minutes', None), parse_int(os.getenv('SUBDOMAINS_RUNNING_TIMEOUT_MINUTES'), 60))\nlock_ttl_minutes = parse_int(read_key(trigger_input, 'pt_lock_ttl_minutes', None), parse_int(os.getenv('PT_LOCK_TTL_MINUTES'), 20))\n\nretry_limits = {\n    'subdomains': parse_int(os.getenv('PT_RETRY_SUBDOMAINS_MAX'), 3),\n    'nmap': parse_int(os.getenv('PT_RETRY_NMAP_MAX'), 3),\n    'targets': parse_int(os.getenv('PT_RETRY_TARGETS_MAX'), 3),\n    'acu': parse_int(os.getenv('PT_RETRY_ACU_MAX'), 3),\n}\n\nrun_lock_owner = str(read_key(trigger_input, 'lock_owner', '') or '').strip()\nif not run_lock_owner:\n    run_lock_owner = f\"dojo-master:{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S%f')}\"\n\nproduct_types = to_list(read_key(incoming, 'product_types', []))\nproducts = to_list(read_key(incoming, 'products', []))\n\npt_list = []\nfor pt in product_types:\n    pt_id = read_key(pt, 'id')\n    try:\n        pt_id = int(pt_id)\n    except Exception:\n        continue\n    if trigger_product_type_id is not None and pt_id != trigger_product_type_id:\n        continue\n    pt_list.append(pt)\n\nnow = datetime.now(timezone.utc)\nlock_until_iso = (now + timedelta(minutes=lock_ttl_minutes)).isoformat()\nacu_dispatch_policy = {\n    'fairness': 'round_robin_by_pt',\n    'node_selection': normalize_node_policy(os.getenv('ACUNETIX_NODE_SELECTION_POLICY')),\n    'sticky_assignment': parse_bool(os.getenv('ACUNETIX_STICKY_ASSIGNMENT', 'true'), True),\n}\nsubdomains_running_count = 0\nsubdomains_running_jobs = 0\nrecovered_stale_subdomains = 0\n\npt_node_mapping = load_pt_node_mapping(PT_NODE_MAPPING_FILE)\nacunetix_nodes = load_acunetix_nodes_from_env()\nfallback_assignment_counts = {}\n\nprepared_pts = []\nfor pt in sorted(pt_list, key=lambda x: int(read_key(x, 'id', 0) or 0)):\n    pt_id = int(read_key(pt, 'id'))\n    original_description = read_key(pt, 'description', '')\n    state_obj = parse_state(original_description) or {}\n    mapped_node = normalize_selected_node(pt_node_mapping.get(str(pt_id)))\n    state = normalize_state(state_obj.get('state'))\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    subdomains_total = int(counters.get('subdomains_total', state_obj.get('subdomains_total', 0)) or 0)\n    subdomains_done = int(counters.get('subdomains_done', state_obj.get('subdomains_done', 0)) or 0)\n    subdomains_failed = int(counters.get('subdomains_failed', state_obj.get('subdomains_failed', 0)) or 0)\n    subdomains_running = int(counters.get('subdomains_running', state_obj.get('subdomains_running', 0)) or 0)\n\n    lock_owner = str(state_obj.get('lock_owner') or '')\n    lock_until_dt = parse_ts(state_obj.get('lock_until'))\n    lock_active = lock_until_dt is not None and lock_until_dt > now and lock_owner and lock_owner != run_lock_owner\n\n    if state == 'subdomains_running':\n        last_update_dt = parse_ts(state_obj.get('last_update'))\n        stale = False\n        if last_update_dt is None:\n            stale = True\n        else:\n            stale = now - last_update_dt > timedelta(minutes=subdomains_stale_minutes)\n\n        if stale:\n            recovered_stale_subdomains += 1\n            retry_count += 1\n            state = 'error'\n            counters['subdomains_running'] = 0\n            updated = {\n                'version': int(state_obj.get('version', 1) or 1),\n                'state': 'error',\n                'counters': counters,\n                'last_update': now.isoformat(),\n                'retry_count': retry_count,\n                'last_error': f'Recovered stale subdomains_running on restart (timeout {subdomains_stale_minutes}m).',\n                'last_stage': 'subdomains',\n                'lock_owner': None,\n                'lock_until': None,\n            }\n            prepared_pts.append({'pt': pt, 'state': state, 'state_obj': updated, 'original_description': original_description, 'lock_active': False, 'mapped_node': mapped_node})\n            continue\n\n        subdomains_running_count += 1\n\n    if state == 'subdomains_running':\n        subdomains_running_jobs += max(1, subdomains_running)\n    prepared_pts.append({'pt': pt, 'state': state, 'state_obj': state_obj, 'original_description': original_description, 'lock_active': lock_active, 'mapped_node': mapped_node})\n\nsubdomains_slots = max(0, subdomains_limit - subdomains_running_jobs)\n\neligible = []\nfor row in prepared_pts:\n    if row.get('lock_active'):\n        continue\n    if row['state'] not in ('new', 'error', 'subdomains_running', 'subdomains_done', 'nmap_done', 'targets_ready'):\n        continue\n    if row['state'] == 'error':\n        state_obj = row.get('state_obj') if isinstance(row.get('state_obj'), dict) else {}\n        stage = str(state_obj.get('last_stage') or 'subdomains')\n        limit = retry_limits.get(stage, 3)\n        retries = int(state_obj.get('retry_count', 0) or 0)\n        if retries >= limit:\n            continue\n    eligible.append(row)\n\nselected_rows = eligible[:pt_window_size]\nselected_ids = set(int(read_key(row['pt'], 'id')) for row in selected_rows)\n\nqueue_wf_a_subdomains_pt = []\nqueue_wf_b_nmap_product = []\nqueue_wf_c_targets_for_pt = []\nqueue_wf_d_pt_acunetixscan = []\nstate_updates = []\n\nfor row in prepared_pts:\n    if row.get('state_obj', {}).get('state') == 'error' and row['state'] == 'error' and 'Recovered stale subdomains_running' in str(row['state_obj'].get('last_error', '')):\n        pt_id = int(read_key(row['pt'], 'id'))\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(row['original_description'], row['state_obj']),\n            'state': 'error',\n        })\n\nfor row in selected_rows:\n    pt = row['pt']\n    pt_id = int(read_key(pt, 'id'))\n    pt_name = read_key(pt, 'name')\n    original_description = row['original_description']\n    state_obj = row['state_obj'] if isinstance(row['state_obj'], dict) else {}\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    current_state = row['state']\n\n    mapped_node = normalize_selected_node(row.get('mapped_node'))\n    state_selected_node = normalize_selected_node(state_obj.get('selected_acu_node'))\n    if state_selected_node is None:\n        state_selected_node = resolve_node_from_reference(state_obj.get('selected_acu_node'), acunetix_nodes)\n    if state_selected_node is None:\n        state_selected_node = resolve_node_from_reference({\n            'endpoint': state_obj.get('acunetix_endpoint'),\n            'name': state_obj.get('acunetix_node_name'),\n        }, acunetix_nodes)\n    resolved_acu_node = state_selected_node or mapped_node\n\n    next_state = None\n    queue_name = None\n    dispatched_subdomain_jobs = 0\n    if current_state in ('new', 'error', 'subdomains_running'):\n        subdomains_total = int(counters.get('subdomains_total', state_obj.get('subdomains_total', 0)) or 0)\n        subdomains_done = int(counters.get('subdomains_done', state_obj.get('subdomains_done', 0)) or 0)\n        subdomains_failed = int(counters.get('subdomains_failed', state_obj.get('subdomains_failed', 0)) or 0)\n        subdomains_running = int(counters.get('subdomains_running', state_obj.get('subdomains_running', 0)) or 0)\n\n        if subdomains_total <= 0:\n            subdomains_total = max(1, subdomains_done + subdomains_failed + subdomains_running + 1)\n\n        pending_subdomain_jobs = max(0, subdomains_total - subdomains_done - subdomains_failed - subdomains_running)\n        if subdomains_slots > 0 and pending_subdomain_jobs > 0:\n            dispatched_subdomain_jobs = min(subdomains_slots, pending_subdomain_jobs)\n            next_state = 'subdomains_running'\n            queue_name = 'a'\n            subdomains_slots -= dispatched_subdomain_jobs\n    elif current_state in ('subdomains_done', 'nmap_running'):\n        queue_name = 'b'\n    elif current_state == 'nmap_done':\n        next_state = 'targets_ready'\n        queue_name = 'c'\n    elif current_state == 'targets_ready':\n        next_state = 'acu_running'\n        queue_name = 'd'\n\n    if queue_name == 'a':\n        for job_idx in range(dispatched_subdomain_jobs):\n            queue_wf_a_subdomains_pt.append({\n                'pt_id': pt_id,\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain,\n                'stage': 'subdomains',\n                'subdomain_job_index': job_idx + 1,\n                'subdomain_job_batch_size': dispatched_subdomain_jobs,\n                'job_metadata': {\n                    'source_workflow': 'WF_Dojo_Master',\n                    'queue': 'wf_a_subdomains_pt',\n                    'transition': f\"{current_state}->subdomains_running\",\n                    'subdomain_job_granularity': 'pt_internal_job',\n                    'lock_owner': run_lock_owner,\n                },\n            })\n    elif queue_name == 'b':\n        jobs = []\n        for product in products:\n            try:\n                product_pt = int(read_key(product, 'prod_type'))\n                product_id = int(read_key(product, 'id'))\n            except Exception:\n                continue\n            if product_pt != pt_id:\n                continue\n            jobs.append({\n                'product_id': product_id,\n                'product_name': read_key(product, 'name'),\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain\n            })\n        jobs = sorted(jobs, key=lambda p: int(read_key(p, 'product_id', 0) or 0))\n\n        nmap_total = int(counters.get('nmap_total', 0) or 0)\n        if nmap_total <= 0:\n            nmap_total = len(jobs)\n        nmap_done = int(counters.get('nmap_done', 0) or 0)\n        nmap_failed = int(counters.get('nmap_failed', 0) or 0)\n\n        processed = max(0, min(nmap_total, nmap_done + nmap_failed))\n        pending_jobs = jobs[processed:]\n        batch = pending_jobs[:nmap_limit]\n\n        counters['nmap_total'] = nmap_total\n        counters['nmap_done'] = nmap_done\n        counters['nmap_failed'] = nmap_failed\n\n        for job in batch:\n            queue_wf_b_nmap_product.append({\n                **job,\n                'pt_id': pt_id,\n                'stage': 'nmap',\n                'job_metadata': {\n                    'source_workflow': 'WF_Dojo_Master',\n                    'queue': 'wf_b_nmap_product',\n                    'transition': f\"{current_state}->nmap_running\",\n                    'lock_owner': run_lock_owner,\n                },\n                'nmap_total': nmap_total,\n                'nmap_done_initial': nmap_done,\n                'nmap_failed_initial': nmap_failed,\n                'nmap_state_description': original_description,\n            })\n\n        if nmap_total == 0 or processed >= nmap_total:\n            next_state = 'nmap_done'\n        elif batch:\n            next_state = 'nmap_running'\n    elif queue_name == 'c':\n        selected_acu_node = resolved_acu_node\n        acunetix_endpoint = (selected_acu_node or {}).get('endpoint') or state_obj.get('acunetix_endpoint')\n        acunetix_api_key = (selected_acu_node or {}).get('api_key') or (selected_acu_node or {}).get('token') or state_obj.get('acunetix_api_key') or state_obj.get('acunetix_token')\n        acunetix_node_name = (selected_acu_node or {}).get('name') or state_obj.get('acunetix_node_name')\n\n        if selected_acu_node is None and acunetix_endpoint and acunetix_api_key:\n            selected_acu_node = normalize_selected_node({\n                'endpoint': acunetix_endpoint,\n                'api_key': acunetix_api_key,\n                'token': acunetix_api_key,\n                'name': acunetix_node_name or 'acu-selected',\n            })\n\n        if selected_acu_node is None:\n            selected_acu_node = select_fallback_acu_node(acunetix_nodes, acu_dispatch_policy.get('node_selection'), fallback_assignment_counts)\n\n        if selected_acu_node:\n            acunetix_endpoint = selected_acu_node.get('endpoint') or acunetix_endpoint\n            acunetix_api_key = selected_acu_node.get('api_key') or selected_acu_node.get('token') or acunetix_api_key\n            acunetix_node_name = selected_acu_node.get('name') or acunetix_node_name\n            resolved_acu_node = selected_acu_node\n\n        queue_wf_c_targets_for_pt.append({\n            'pt_id': pt_id,\n            'product_type_id': pt_id,\n            'product_type_name': pt_name,\n            'domain': trigger_domain,\n            'stage': 'targets',\n            'selected_acu_node': selected_acu_node,\n            'acunetix_endpoint': acunetix_endpoint,\n            'acunetix_api_key': acunetix_api_key,\n            'acunetix_token': acunetix_api_key,\n            'acunetix_node_name': acunetix_node_name,\n            'job_metadata': {\n                'source_workflow': 'WF_Dojo_Master',\n                'queue': 'wf_c_targets_for_pt',\n                'transition': f\"{current_state}->targets_ready\",\n                'lock_owner': run_lock_owner,\n            },\n        })\n    elif queue_name == 'd':\n        queue_wf_d_pt_acunetixscan.append({\n            'pt_id': pt_id,\n            'product_type_id': pt_id,\n            'product_type_name': pt_name,\n            'domain': trigger_domain,\n            'stage': 'acu',\n            'selected_acu_node': resolved_acu_node,\n            'dispatch_policy': acu_dispatch_policy,\n            'job_metadata': {\n                'source_workflow': 'WF_Dojo_Master',\n                'queue': 'wf_d_pt_acunetixscan',\n                'transition': f\"{current_state}->acu_running\",\n                'lock_owner': run_lock_owner,\n            },\n        })\n\n    if next_state:\n        if next_state == 'subdomains_running':\n            counters['subdomains_total'] = int(counters.get('subdomains_total', 0) or 0)\n            counters['subdomains_done'] = int(counters.get('subdomains_done', 0) or 0)\n            counters['subdomains_failed'] = int(counters.get('subdomains_failed', 0) or 0)\n            counters['subdomains_running'] = int(counters.get('subdomains_running', 0) or 0) + max(0, dispatched_subdomain_jobs)\n            min_total = counters['subdomains_done'] + counters['subdomains_failed'] + counters['subdomains_running']\n            if counters['subdomains_total'] < min_total:\n                counters['subdomains_total'] = min_total\n\n        stage_key = {\n            'subdomains_running': 'subdomains_runs',\n            'nmap_running': 'nmap_runs',\n            'targets_ready': 'targets_runs',\n            'acu_running': 'acu_runs',\n        }.get(next_state)\n        if stage_key:\n            stage_increment = max(1, dispatched_subdomain_jobs) if next_state == 'subdomains_running' else 1\n            counters[stage_key] = int(counters.get(stage_key, 0) or 0) + stage_increment\n\n        last_stage = {\n            'subdomains_running': 'subdomains',\n            'nmap_running': 'nmap',\n            'targets_ready': 'targets',\n            'acu_running': 'acu',\n        }.get(next_state, state_obj.get('last_stage'))\n\n        updated = {\n            'version': int(state_obj.get('version', 1) or 1),\n            'state': next_state,\n            'counters': counters,\n            'subdomains_total': int(counters.get('subdomains_total', 0) or 0),\n            'subdomains_done': int(counters.get('subdomains_done', 0) or 0),\n            'subdomains_failed': int(counters.get('subdomains_failed', 0) or 0),\n            'subdomains_running': int(counters.get('subdomains_running', 0) or 0),\n            'last_update': now.isoformat(),\n            'retry_count': retry_count,\n            'last_error': None if next_state == 'subdomains_running' else state_obj.get('last_error'),\n            'last_stage': last_stage,\n            'lock_owner': run_lock_owner if next_state in {'subdomains_running', 'nmap_running', 'targets_ready', 'acu_running'} else None,\n            'lock_until': lock_until_iso if next_state in {'subdomains_running', 'nmap_running', 'targets_ready', 'acu_running'} else None,\n        }\n        if next_state in {'targets_ready', 'acu_running'}:\n            bound_node = resolved_acu_node\n            if bound_node:\n                updated['selected_acu_node'] = {\n                    'endpoint': bound_node.get('endpoint'),\n                    'name': bound_node.get('name'),\n                }\n                updated['acunetix_endpoint'] = bound_node.get('endpoint')\n                updated['acunetix_node_name'] = bound_node.get('name')\n        if next_state == 'acu_running':\n            updated['acu_dispatch_policy'] = acu_dispatch_policy\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(original_description, updated),\n            'state': next_state,\n        })\n\n\ndef build_targets_stage_contract_smoke(queue_rows):\n    invalid = []\n    for idx, row in enumerate(to_list(queue_rows)):\n        if not isinstance(row, dict):\n            invalid.append({'index': idx, 'missing_fields': list(TARGETS_STAGE_CONTRACT_FIELDS)})\n            continue\n        missing = [field for field in TARGETS_STAGE_CONTRACT_FIELDS if field not in row]\n        if missing:\n            invalid.append({'index': idx, 'missing_fields': missing})\n    return {\n        'stage': 'targets',\n        'required_fields': list(TARGETS_STAGE_CONTRACT_FIELDS),\n        'queue_size': len(to_list(queue_rows)),\n        'ok': len(invalid) == 0,\n        'invalid_rows': invalid,\n    }\n\nreturn [{\n    'json': {\n        'run_subdomains': len(queue_wf_a_subdomains_pt) > 0,\n        'run_nmap': len(queue_wf_b_nmap_product) > 0,\n        'run_targets': len(queue_wf_c_targets_for_pt) > 0,\n        'run_acunetix': len(queue_wf_d_pt_acunetixscan) > 0,\n        'queue_wf_a_subdomains_pt': queue_wf_a_subdomains_pt,\n        'queue_wf_b_nmap_product': queue_wf_b_nmap_product,\n        'queue_wf_c_targets_for_pt': queue_wf_c_targets_for_pt,\n        'queue_wf_d_pt_acunetixscan': queue_wf_d_pt_acunetixscan,\n        'state_updates': state_updates,\n        'trigger_product_type_id': trigger_product_type_id,\n        'trigger_domain': trigger_domain,\n        'pt_window_size': pt_window_size,\n        'selected_pt_ids': sorted(list(selected_ids)),\n        'subdomains_concurrency': subdomains_limit,\n        'subdomains_running_now': subdomains_running_count,\n        'subdomains_running_jobs': subdomains_running_jobs,\n        'subdomains_slots_available': max(0, subdomains_limit - subdomains_running_jobs),\n        'subdomains_stale_recovered': recovered_stale_subdomains,\n        'subdomains_running_timeout_minutes': subdomains_stale_minutes,\n        'nmap_concurrency': nmap_limit,\n        'pt_lock_ttl_minutes': lock_ttl_minutes,\n        'lock_owner': run_lock_owner,\n        'retry_limits': retry_limits,\n        'stage_barriers': {\n            'subdomains_done_after_all_jobs': True,\n            'nmap_done_after_all_jobs': True,\n        },\n        'acu_dispatch_policy': acu_dispatch_policy,\n        'targets_stage_contract_smoke': build_targets_stage_contract_smoke(queue_wf_c_targets_for_pt),\n    }\n}]"
      },
      "id": "m-plan",
      "name": "Build Plan (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -950,
        100
      ]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_subdomains}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-subdomains",
      "name": "Need WF_A_Subdomains_PT?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -760,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_A_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-a",
      "name": "Run WF_A_Subdomains_PT",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        -460,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_nmap}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-nmap",
      "name": "Need WF_B_Nmap_Product?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -340,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_B_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-b",
      "name": "Run WF_B_Nmap_Product",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        -40,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_targets}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-targets",
      "name": "Need WF_C_Targets_For_PT?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        60,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_C_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-c",
      "name": "Run WF_C_Targets_For_PT",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        360,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.run_acunetix}}",
              "operation": "equal",
              "value2": true
            }
          ]
        }
      },
      "id": "m-if-acu",
      "name": "Need WF_D_PT_AcunetixScan?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        460,
        100
      ]
    },
    {
      "parameters": {
        "workflowId": "={{ $env.N8N_WF_D_ID }}",
        "options": {
          "waitForSubWorkflow": true
        }
      },
      "id": "m-run-d",
      "name": "Run WF_D_PT_AcunetixScan",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1,
      "position": [
        670,
        20
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_a_subdomains_pt', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['pt_id'] = read_key(row, 'pt_id', read_key(row, 'product_type_id'))\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    item['stage'] = read_key(row, 'stage', 'subdomains')\n    item['job_metadata'] = read_key(row, 'job_metadata', {})\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-a",
      "name": "Expand WF_A Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -660,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_b_nmap_product', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['pt_id'] = read_key(row, 'pt_id', read_key(row, 'product_type_id'))\n    item['product_id'] = read_key(row, 'product_id')\n    item['product_name'] = read_key(row, 'product_name')\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    item['stage'] = read_key(row, 'stage', 'nmap')\n    item['job_metadata'] = read_key(row, 'job_metadata', {})\n    item['nmap_total'] = read_key(row, 'nmap_total')\n    item['nmap_done_initial'] = read_key(row, 'nmap_done_initial')\n    item['nmap_failed_initial'] = read_key(row, 'nmap_failed_initial')\n    item['nmap_state_description'] = read_key(row, 'nmap_state_description')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-b",
      "name": "Expand WF_B Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -240,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_c_targets_for_pt', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['pt_id'] = read_key(row, 'pt_id', read_key(row, 'product_type_id'))\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    item['stage'] = read_key(row, 'stage', 'targets')\n    item['job_metadata'] = read_key(row, 'job_metadata', {})\n    item['selected_acu_node'] = read_key(row, 'selected_acu_node')\n    item['acunetix_node_name'] = read_key(row, 'acunetix_node_name')\n    item['acunetix_endpoint'] = read_key(row, 'acunetix_endpoint')\n    item['acunetix_api_key'] = read_key(row, 'acunetix_api_key')\n    item['acunetix_token'] = read_key(row, 'acunetix_token')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-c",
      "name": "Expand WF_C Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        160,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else None\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    try:\n        first = value[0]\n        if hasattr(first, 'get') or hasattr(first, '__getitem__'):\n            return first\n    except Exception:\n        pass\n    return value\n\ndef is_mapping_like(value):\n    return hasattr(value, 'get') or hasattr(value, '__getitem__')\n\nincoming = to_container(payload)\nout = []\nfor row in to_list(read_key(incoming, 'queue_wf_d_pt_acunetixscan', [])):\n    if not is_mapping_like(row):\n        continue\n    item = {}\n    item['pt_id'] = read_key(row, 'pt_id', read_key(row, 'product_type_id'))\n    item['product_type_id'] = read_key(row, 'product_type_id')\n    item['product_type_name'] = read_key(row, 'product_type_name')\n    item['domain'] = read_key(row, 'domain')\n    item['stage'] = read_key(row, 'stage', 'acu')\n    item['job_metadata'] = read_key(row, 'job_metadata', {})\n    item['dispatch_policy'] = read_key(row, 'dispatch_policy', {})\n    item['selected_acu_node'] = read_key(row, 'selected_acu_node')\n    out.append({'json': item})\nreturn out"
      },
      "id": "m-expand-d",
      "name": "Expand WF_D Candidates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        560,
        20
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\nitem = first_item.json if first_item else {}\nif not isinstance(item, dict):\n    item = {}\nreturn [{'json': {'trigger': item}}]"
      },
      "id": "m-prepare-trigger",
      "name": "Prepare Trigger Input (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1170,
        -80
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'product_types': aggregated}}]"
      },
      "id": "m-prepare-pt",
      "name": "Prepare Product Types (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -960,
        -80
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'products': aggregated}}]"
      },
      "id": "m-prepare-products",
      "name": "Prepare Products (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -740,
        280
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-trigger-pt",
      "name": "Merge Trigger + Product Types",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        -740,
        -80
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-build-input",
      "name": "Merge Build Plan Input",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        -520,
        100
      ]
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-product-types-after-a",
      "name": "Get Product Types After WF_A",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -300,
        -260
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/products/?limit=200&offset=0' }}",
        "options": {}
      },
      "id": "m-get-products-after-a",
      "name": "Get Products After WF_A",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -300,
        -120
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'product_types': aggregated}}]"
      },
      "id": "m-prepare-pt-after-a",
      "name": "Prepare Product Types After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -260
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import os\nimport json\nimport urllib.request\n\nitems = _input.all()\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef fetch_next_pages(next_url):\n    token = (os.environ.get('DOJO_API_TOKEN') or '').strip()\n    if not next_url or not token:\n        return []\n\n    out = []\n    visited = set()\n    current = str(next_url)\n\n    while current and current not in visited:\n        visited.add(current)\n        req = urllib.request.Request(current)\n        req.add_header('Authorization', f'Token {token}')\n        req.add_header('Accept', 'application/json')\n        with urllib.request.urlopen(req, timeout=30) as resp:\n            payload = json.loads(resp.read().decode('utf-8') or '{}')\n        out.extend(to_list(read_key(payload, 'results', [])))\n        current = read_key(payload, 'next')\n\n    return out\n\n\naggregated = []\nnext_url = None\nfor item in items:\n    payload = item.json if item else None\n    if payload is None:\n        continue\n    aggregated.extend(to_list(read_key(payload, 'results', [])))\n    if next_url is None:\n        next_url = read_key(payload, 'next')\n\naggregated.extend(fetch_next_pages(next_url))\n\nreturn [{'json': {'products': aggregated}}]"
      },
      "id": "m-prepare-products-after-a",
      "name": "Prepare Products After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -60,
        -120
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-trigger-pt-after-a",
      "name": "Merge Trigger + Product Types After WF_A",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        150,
        -220
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "id": "m-merge-rebuild-input",
      "name": "Merge Rebuild Plan Input",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        340,
        -140
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport os\nimport re\nfrom datetime import datetime, timezone, timedelta\n\nPT_NODE_MAPPING_FILE = (os.environ.get('ACUNETIX_PT_NODE_MAPPING_FILE') or '/tmp/acunetix_pt_node_mapping.json').strip()\n\nfirst_item = _input.first()\npayload = first_item.json if first_item else None\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\nVALID_STATES = {\n    'new', 'subdomains_running', 'subdomains_done',\n    'nmap_running', 'nmap_done', 'targets_ready',\n    'acu_running', 'done', 'error'\n}\n\nTARGETS_STAGE_CONTRACT_FIELDS = [\n    'pt_id', 'product_type_id', 'product_type_name', 'domain', 'stage',\n    'selected_acu_node', 'acunetix_endpoint', 'acunetix_api_key', 'acunetix_token', 'acunetix_node_name',\n    'job_metadata',\n]\n\n\ndef read_key(container, key, default=None):\n    try:\n        value = container.get(key)\n        return default if value is None else value\n    except Exception:\n        try:\n            value = container[key]\n            return default if value is None else value\n        except Exception:\n            return default\n\n\ndef to_list(value):\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return value\n    try:\n        return [v for v in value]\n    except Exception:\n        return []\n\n\ndef to_container(value):\n    if value is None:\n        return {}\n    if isinstance(value, list):\n        return value[0] if value else {}\n    return value\n\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return None\n    pattern = re.compile(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", re.DOTALL)\n    m = pattern.search(description)\n    if not m:\n        return None\n    try:\n        data = json.loads(m.group(1).strip())\n        if not isinstance(data, dict):\n            return None\n        return data\n    except Exception:\n        return None\n\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\n\ndef normalize_state(raw_state):\n    state = str(raw_state or 'new')\n    return state if state in VALID_STATES else 'new'\n\n\ndef parse_int(value, default):\n    try:\n        val = int(value)\n        return val if val > 0 else default\n    except Exception:\n        return default\n\n\ndef parse_bool(value, default=False):\n    if value is None:\n        return default\n    if isinstance(value, bool):\n        return value\n    raw = str(value).strip().lower()\n    if raw in {'1', 'true', 'yes', 'y', 'on'}:\n        return True\n    if raw in {'0', 'false', 'no', 'n', 'off'}:\n        return False\n    return default\n\n\ndef normalize_node_policy(value):\n    raw = str(value or '').strip().lower()\n    return raw if raw in {'least_loaded', 'weighted'} else 'least_loaded'\n\n\n\ndef load_pt_node_mapping(path):\n    if not path:\n        return {}\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            payload = json.load(f)\n        items = payload.get('items', {}) if isinstance(payload, dict) else {}\n        return items if isinstance(items, dict) else {}\n    except Exception:\n        return {}\n\n\ndef normalize_selected_node(raw):\n    if not isinstance(raw, dict):\n        return None\n    endpoint = str(raw.get('endpoint') or '').strip().rstrip('/')\n    token = str(raw.get('api_key') or raw.get('token') or '').strip()\n    if not endpoint or not token:\n        return None\n    return {\n        'endpoint': endpoint,\n        'api_key': token,\n        'token': token,\n        'name': str(raw.get('name') or 'acu-selected').strip() or 'acu-selected',\n        'weight': parse_int(raw.get('weight', 1), 1),\n    }\n\n\ndef normalize_node_reference(raw):\n    if not isinstance(raw, dict):\n        return None\n    endpoint = str(raw.get('endpoint') or '').strip().rstrip('/')\n    name = str(raw.get('name') or '').strip()\n    if not endpoint and not name:\n        return None\n    return {\n        'endpoint': endpoint,\n        'name': name,\n    }\n\n\ndef resolve_node_from_reference(raw, nodes):\n    ref = normalize_node_reference(raw)\n    if not ref:\n        return None\n    endpoint = ref.get('endpoint')\n    name = ref.get('name')\n    for node in nodes:\n        if endpoint and str(node.get('endpoint') or '').strip().rstrip('/') == endpoint:\n            return normalize_selected_node(node)\n    for node in nodes:\n        if name and str(node.get('name') or '').strip() == name:\n            return normalize_selected_node(node)\n    return None\n\n\ndef load_acunetix_nodes_from_env():\n    nodes = []\n    raw = (os.environ.get('ACUNETIX_INSTANCES_JSON') or '').strip()\n    if raw:\n        try:\n            arr = json.loads(raw)\n            if isinstance(arr, list):\n                for idx, item in enumerate(arr):\n                    node = normalize_selected_node(item)\n                    if node:\n                        if not str(node.get('name') or '').strip():\n                            node['name'] = f'acu-{idx + 1}'\n                        nodes.append(node)\n        except Exception:\n            nodes = []\n\n    if nodes:\n        return nodes\n\n    endpoint = str(os.environ.get('ACUNETIX_BASE_URL') or 'https://localhost:3443').strip().rstrip('/')\n    token = str((os.environ.get('ACUNETIX_API_KEY') or '').strip() or (os.environ.get('ACU_API_TOKEN') or '').strip())\n    fallback = normalize_selected_node({\n        'name': 'acu-default',\n        'endpoint': endpoint,\n        'api_key': token,\n    })\n    return [fallback] if fallback else []\n\n\ndef select_fallback_acu_node(nodes, node_policy, fallback_counts):\n    if not nodes:\n        return None\n    candidates = []\n    for node in nodes:\n        normalized = normalize_selected_node(node)\n        if not normalized:\n            continue\n        endpoint = normalized.get('endpoint')\n        if not endpoint:\n            continue\n        count = int(fallback_counts.get(endpoint, 0) or 0)\n        weight = parse_int(node.get('weight', normalized.get('weight', 1)), 1)\n        candidates.append({\n            'node': normalized,\n            'endpoint': endpoint,\n            'count': count,\n            'weight': weight,\n            'name': normalized.get('name') or '',\n        })\n    if not candidates:\n        return None\n\n    if node_policy == 'weighted':\n        chosen = sorted(candidates, key=lambda c: (c['count'] / max(1, c['weight']), c['count'], c['name']))[0]\n    else:\n        chosen = sorted(candidates, key=lambda c: (c['count'], c['name']))[0]\n\n    fallback_counts[chosen['endpoint']] = int(fallback_counts.get(chosen['endpoint'], 0) or 0) + 1\n    return chosen['node']\n\n\ndef parse_ts(value):\n    if not isinstance(value, str) or not value.strip():\n        return None\n    raw = value.strip()\n    if raw.endswith('Z'):\n        raw = raw[:-1] + '+00:00'\n    try:\n        dt = datetime.fromisoformat(raw)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt.astimezone(timezone.utc)\n    except Exception:\n        return None\n\n\nincoming = to_container(payload)\ntrigger_input = to_container(read_key(incoming, 'trigger', {}))\ntrigger_product_type_id = read_key(trigger_input, 'product_type_id')\ntrigger_domain = read_key(trigger_input, 'domain')\n\ntry:\n    trigger_product_type_id = int(trigger_product_type_id) if trigger_product_type_id is not None else None\nexcept Exception:\n    trigger_product_type_id = None\n\npt_window_size = parse_int(read_key(trigger_input, 'pt_window_size', None), parse_int(os.getenv('PT_WINDOW_SIZE'), 1))\nsubdomains_limit = parse_int(read_key(trigger_input, 'subdomains_concurrency', None), parse_int(os.getenv('SUBDOMAINS_CONCURRENCY'), 5))\nnmap_limit = parse_int(read_key(trigger_input, 'nmap_concurrency', None), parse_int(os.getenv('NMAP_CONCURRENCY'), 5))\nsubdomains_stale_minutes = parse_int(read_key(trigger_input, 'subdomains_running_timeout_minutes', None), parse_int(os.getenv('SUBDOMAINS_RUNNING_TIMEOUT_MINUTES'), 60))\nlock_ttl_minutes = parse_int(read_key(trigger_input, 'pt_lock_ttl_minutes', None), parse_int(os.getenv('PT_LOCK_TTL_MINUTES'), 20))\n\nretry_limits = {\n    'subdomains': parse_int(os.getenv('PT_RETRY_SUBDOMAINS_MAX'), 3),\n    'nmap': parse_int(os.getenv('PT_RETRY_NMAP_MAX'), 3),\n    'targets': parse_int(os.getenv('PT_RETRY_TARGETS_MAX'), 3),\n    'acu': parse_int(os.getenv('PT_RETRY_ACU_MAX'), 3),\n}\n\nrun_lock_owner = str(read_key(trigger_input, 'lock_owner', '') or '').strip()\nif not run_lock_owner:\n    run_lock_owner = f\"dojo-master:{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S%f')}\"\n\nproduct_types = to_list(read_key(incoming, 'product_types', []))\nproducts = to_list(read_key(incoming, 'products', []))\n\npt_list = []\nfor pt in product_types:\n    pt_id = read_key(pt, 'id')\n    try:\n        pt_id = int(pt_id)\n    except Exception:\n        continue\n    if trigger_product_type_id is not None and pt_id != trigger_product_type_id:\n        continue\n    pt_list.append(pt)\n\nnow = datetime.now(timezone.utc)\nlock_until_iso = (now + timedelta(minutes=lock_ttl_minutes)).isoformat()\nacu_dispatch_policy = {\n    'fairness': 'round_robin_by_pt',\n    'node_selection': normalize_node_policy(os.getenv('ACUNETIX_NODE_SELECTION_POLICY')),\n    'sticky_assignment': parse_bool(os.getenv('ACUNETIX_STICKY_ASSIGNMENT', 'true'), True),\n}\nsubdomains_running_count = 0\nsubdomains_running_jobs = 0\nrecovered_stale_subdomains = 0\n\npt_node_mapping = load_pt_node_mapping(PT_NODE_MAPPING_FILE)\nacunetix_nodes = load_acunetix_nodes_from_env()\nfallback_assignment_counts = {}\n\nprepared_pts = []\nfor pt in sorted(pt_list, key=lambda x: int(read_key(x, 'id', 0) or 0)):\n    pt_id = int(read_key(pt, 'id'))\n    original_description = read_key(pt, 'description', '')\n    state_obj = parse_state(original_description) or {}\n    mapped_node = normalize_selected_node(pt_node_mapping.get(str(pt_id)))\n    state = normalize_state(state_obj.get('state'))\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    subdomains_total = int(counters.get('subdomains_total', state_obj.get('subdomains_total', 0)) or 0)\n    subdomains_done = int(counters.get('subdomains_done', state_obj.get('subdomains_done', 0)) or 0)\n    subdomains_failed = int(counters.get('subdomains_failed', state_obj.get('subdomains_failed', 0)) or 0)\n    subdomains_running = int(counters.get('subdomains_running', state_obj.get('subdomains_running', 0)) or 0)\n\n    lock_owner = str(state_obj.get('lock_owner') or '')\n    lock_until_dt = parse_ts(state_obj.get('lock_until'))\n    lock_active = lock_until_dt is not None and lock_until_dt > now and lock_owner and lock_owner != run_lock_owner\n\n    if state == 'subdomains_running':\n        last_update_dt = parse_ts(state_obj.get('last_update'))\n        stale = False\n        if last_update_dt is None:\n            stale = True\n        else:\n            stale = now - last_update_dt > timedelta(minutes=subdomains_stale_minutes)\n\n        if stale:\n            recovered_stale_subdomains += 1\n            retry_count += 1\n            state = 'error'\n            counters['subdomains_running'] = 0\n            updated = {\n                'version': int(state_obj.get('version', 1) or 1),\n                'state': 'error',\n                'counters': counters,\n                'last_update': now.isoformat(),\n                'retry_count': retry_count,\n                'last_error': f'Recovered stale subdomains_running on restart (timeout {subdomains_stale_minutes}m).',\n                'last_stage': 'subdomains',\n                'lock_owner': None,\n                'lock_until': None,\n            }\n            prepared_pts.append({'pt': pt, 'state': state, 'state_obj': updated, 'original_description': original_description, 'lock_active': False, 'mapped_node': mapped_node})\n            continue\n\n        subdomains_running_count += 1\n\n    if state == 'subdomains_running':\n        subdomains_running_jobs += max(1, subdomains_running)\n    prepared_pts.append({'pt': pt, 'state': state, 'state_obj': state_obj, 'original_description': original_description, 'lock_active': lock_active, 'mapped_node': mapped_node})\n\nsubdomains_slots = max(0, subdomains_limit - subdomains_running_jobs)\n\neligible = []\nfor row in prepared_pts:\n    if row.get('lock_active'):\n        continue\n    if row['state'] not in ('new', 'error', 'subdomains_running', 'subdomains_done', 'nmap_done', 'targets_ready'):\n        continue\n    if row['state'] == 'error':\n        state_obj = row.get('state_obj') if isinstance(row.get('state_obj'), dict) else {}\n        stage = str(state_obj.get('last_stage') or 'subdomains')\n        limit = retry_limits.get(stage, 3)\n        retries = int(state_obj.get('retry_count', 0) or 0)\n        if retries >= limit:\n            continue\n    eligible.append(row)\n\nselected_rows = eligible[:pt_window_size]\nselected_ids = set(int(read_key(row['pt'], 'id')) for row in selected_rows)\n\nqueue_wf_a_subdomains_pt = []\nqueue_wf_b_nmap_product = []\nqueue_wf_c_targets_for_pt = []\nqueue_wf_d_pt_acunetixscan = []\nstate_updates = []\n\nfor row in prepared_pts:\n    if row.get('state_obj', {}).get('state') == 'error' and row['state'] == 'error' and 'Recovered stale subdomains_running' in str(row['state_obj'].get('last_error', '')):\n        pt_id = int(read_key(row['pt'], 'id'))\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(row['original_description'], row['state_obj']),\n            'state': 'error',\n        })\n\nfor row in selected_rows:\n    pt = row['pt']\n    pt_id = int(read_key(pt, 'id'))\n    pt_name = read_key(pt, 'name')\n    original_description = row['original_description']\n    state_obj = row['state_obj'] if isinstance(row['state_obj'], dict) else {}\n    counters = state_obj.get('counters') if isinstance(state_obj.get('counters'), dict) else {}\n    retry_count = int(state_obj.get('retry_count', 0) or 0)\n    current_state = row['state']\n\n    mapped_node = normalize_selected_node(row.get('mapped_node'))\n    state_selected_node = normalize_selected_node(state_obj.get('selected_acu_node'))\n    if state_selected_node is None:\n        state_selected_node = resolve_node_from_reference(state_obj.get('selected_acu_node'), acunetix_nodes)\n    if state_selected_node is None:\n        state_selected_node = resolve_node_from_reference({\n            'endpoint': state_obj.get('acunetix_endpoint'),\n            'name': state_obj.get('acunetix_node_name'),\n        }, acunetix_nodes)\n    resolved_acu_node = state_selected_node or mapped_node\n\n    next_state = None\n    queue_name = None\n    dispatched_subdomain_jobs = 0\n    if current_state in ('new', 'error', 'subdomains_running'):\n        subdomains_total = int(counters.get('subdomains_total', state_obj.get('subdomains_total', 0)) or 0)\n        subdomains_done = int(counters.get('subdomains_done', state_obj.get('subdomains_done', 0)) or 0)\n        subdomains_failed = int(counters.get('subdomains_failed', state_obj.get('subdomains_failed', 0)) or 0)\n        subdomains_running = int(counters.get('subdomains_running', state_obj.get('subdomains_running', 0)) or 0)\n\n        if subdomains_total <= 0:\n            subdomains_total = max(1, subdomains_done + subdomains_failed + subdomains_running + 1)\n\n        pending_subdomain_jobs = max(0, subdomains_total - subdomains_done - subdomains_failed - subdomains_running)\n        if subdomains_slots > 0 and pending_subdomain_jobs > 0:\n            dispatched_subdomain_jobs = min(subdomains_slots, pending_subdomain_jobs)\n            next_state = 'subdomains_running'\n            queue_name = 'a'\n            subdomains_slots -= dispatched_subdomain_jobs\n    elif current_state in ('subdomains_done', 'nmap_running'):\n        queue_name = 'b'\n    elif current_state == 'nmap_done':\n        next_state = 'targets_ready'\n        queue_name = 'c'\n    elif current_state == 'targets_ready':\n        next_state = 'acu_running'\n        queue_name = 'd'\n\n    if queue_name == 'a':\n        for job_idx in range(dispatched_subdomain_jobs):\n            queue_wf_a_subdomains_pt.append({\n                'pt_id': pt_id,\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain,\n                'stage': 'subdomains',\n                'subdomain_job_index': job_idx + 1,\n                'subdomain_job_batch_size': dispatched_subdomain_jobs,\n                'job_metadata': {\n                    'source_workflow': 'WF_Dojo_Master',\n                    'queue': 'wf_a_subdomains_pt',\n                    'transition': f\"{current_state}->subdomains_running\",\n                    'subdomain_job_granularity': 'pt_internal_job',\n                    'lock_owner': run_lock_owner,\n                },\n            })\n    elif queue_name == 'b':\n        jobs = []\n        for product in products:\n            try:\n                product_pt = int(read_key(product, 'prod_type'))\n                product_id = int(read_key(product, 'id'))\n            except Exception:\n                continue\n            if product_pt != pt_id:\n                continue\n            jobs.append({\n                'product_id': product_id,\n                'product_name': read_key(product, 'name'),\n                'product_type_id': pt_id,\n                'product_type_name': pt_name,\n                'domain': trigger_domain\n            })\n        jobs = sorted(jobs, key=lambda p: int(read_key(p, 'product_id', 0) or 0))\n\n        nmap_total = int(counters.get('nmap_total', 0) or 0)\n        if nmap_total <= 0:\n            nmap_total = len(jobs)\n        nmap_done = int(counters.get('nmap_done', 0) or 0)\n        nmap_failed = int(counters.get('nmap_failed', 0) or 0)\n\n        processed = max(0, min(nmap_total, nmap_done + nmap_failed))\n        pending_jobs = jobs[processed:]\n        batch = pending_jobs[:nmap_limit]\n\n        counters['nmap_total'] = nmap_total\n        counters['nmap_done'] = nmap_done\n        counters['nmap_failed'] = nmap_failed\n\n        for job in batch:\n            queue_wf_b_nmap_product.append({\n                **job,\n                'pt_id': pt_id,\n                'stage': 'nmap',\n                'job_metadata': {\n                    'source_workflow': 'WF_Dojo_Master',\n                    'queue': 'wf_b_nmap_product',\n                    'transition': f\"{current_state}->nmap_running\",\n                    'lock_owner': run_lock_owner,\n                },\n                'nmap_total': nmap_total,\n                'nmap_done_initial': nmap_done,\n                'nmap_failed_initial': nmap_failed,\n                'nmap_state_description': original_description,\n            })\n\n        if nmap_total == 0 or processed >= nmap_total:\n            next_state = 'nmap_done'\n        elif batch:\n            next_state = 'nmap_running'\n    elif queue_name == 'c':\n        selected_acu_node = resolved_acu_node\n        acunetix_endpoint = (selected_acu_node or {}).get('endpoint') or state_obj.get('acunetix_endpoint')\n        acunetix_api_key = (selected_acu_node or {}).get('api_key') or (selected_acu_node or {}).get('token') or state_obj.get('acunetix_api_key') or state_obj.get('acunetix_token')\n        acunetix_node_name = (selected_acu_node or {}).get('name') or state_obj.get('acunetix_node_name')\n\n        if selected_acu_node is None and acunetix_endpoint and acunetix_api_key:\n            selected_acu_node = normalize_selected_node({\n                'endpoint': acunetix_endpoint,\n                'api_key': acunetix_api_key,\n                'token': acunetix_api_key,\n                'name': acunetix_node_name or 'acu-selected',\n            })\n\n        if selected_acu_node is None:\n            selected_acu_node = select_fallback_acu_node(acunetix_nodes, acu_dispatch_policy.get('node_selection'), fallback_assignment_counts)\n\n        if selected_acu_node:\n            acunetix_endpoint = selected_acu_node.get('endpoint') or acunetix_endpoint\n            acunetix_api_key = selected_acu_node.get('api_key') or selected_acu_node.get('token') or acunetix_api_key\n            acunetix_node_name = selected_acu_node.get('name') or acunetix_node_name\n            resolved_acu_node = selected_acu_node\n\n        queue_wf_c_targets_for_pt.append({\n            'pt_id': pt_id,\n            'product_type_id': pt_id,\n            'product_type_name': pt_name,\n            'domain': trigger_domain,\n            'stage': 'targets',\n            'selected_acu_node': selected_acu_node,\n            'acunetix_endpoint': acunetix_endpoint,\n            'acunetix_api_key': acunetix_api_key,\n            'acunetix_token': acunetix_api_key,\n            'acunetix_node_name': acunetix_node_name,\n            'job_metadata': {\n                'source_workflow': 'WF_Dojo_Master',\n                'queue': 'wf_c_targets_for_pt',\n                'transition': f\"{current_state}->targets_ready\",\n                'lock_owner': run_lock_owner,\n            },\n        })\n    elif queue_name == 'd':\n        queue_wf_d_pt_acunetixscan.append({\n            'pt_id': pt_id,\n            'product_type_id': pt_id,\n            'product_type_name': pt_name,\n            'domain': trigger_domain,\n            'stage': 'acu',\n            'selected_acu_node': resolved_acu_node,\n            'dispatch_policy': acu_dispatch_policy,\n            'job_metadata': {\n                'source_workflow': 'WF_Dojo_Master',\n                'queue': 'wf_d_pt_acunetixscan',\n                'transition': f\"{current_state}->acu_running\",\n                'lock_owner': run_lock_owner,\n            },\n        })\n\n    if next_state:\n        if next_state == 'subdomains_running':\n            counters['subdomains_total'] = int(counters.get('subdomains_total', 0) or 0)\n            counters['subdomains_done'] = int(counters.get('subdomains_done', 0) or 0)\n            counters['subdomains_failed'] = int(counters.get('subdomains_failed', 0) or 0)\n            counters['subdomains_running'] = int(counters.get('subdomains_running', 0) or 0) + max(0, dispatched_subdomain_jobs)\n            min_total = counters['subdomains_done'] + counters['subdomains_failed'] + counters['subdomains_running']\n            if counters['subdomains_total'] < min_total:\n                counters['subdomains_total'] = min_total\n\n        stage_key = {\n            'subdomains_running': 'subdomains_runs',\n            'nmap_running': 'nmap_runs',\n            'targets_ready': 'targets_runs',\n            'acu_running': 'acu_runs',\n        }.get(next_state)\n        if stage_key:\n            stage_increment = max(1, dispatched_subdomain_jobs) if next_state == 'subdomains_running' else 1\n            counters[stage_key] = int(counters.get(stage_key, 0) or 0) + stage_increment\n\n        last_stage = {\n            'subdomains_running': 'subdomains',\n            'nmap_running': 'nmap',\n            'targets_ready': 'targets',\n            'acu_running': 'acu',\n        }.get(next_state, state_obj.get('last_stage'))\n\n        updated = {\n            'version': int(state_obj.get('version', 1) or 1),\n            'state': next_state,\n            'counters': counters,\n            'subdomains_total': int(counters.get('subdomains_total', 0) or 0),\n            'subdomains_done': int(counters.get('subdomains_done', 0) or 0),\n            'subdomains_failed': int(counters.get('subdomains_failed', 0) or 0),\n            'subdomains_running': int(counters.get('subdomains_running', 0) or 0),\n            'last_update': now.isoformat(),\n            'retry_count': retry_count,\n            'last_error': None if next_state == 'subdomains_running' else state_obj.get('last_error'),\n            'last_stage': last_stage,\n            'lock_owner': run_lock_owner if next_state in {'subdomains_running', 'nmap_running', 'targets_ready', 'acu_running'} else None,\n            'lock_until': lock_until_iso if next_state in {'subdomains_running', 'nmap_running', 'targets_ready', 'acu_running'} else None,\n        }\n        if next_state in {'targets_ready', 'acu_running'}:\n            bound_node = resolved_acu_node\n            if bound_node:\n                updated['selected_acu_node'] = {\n                    'endpoint': bound_node.get('endpoint'),\n                    'name': bound_node.get('name'),\n                }\n                updated['acunetix_endpoint'] = bound_node.get('endpoint')\n                updated['acunetix_node_name'] = bound_node.get('name')\n        if next_state == 'acu_running':\n            updated['acu_dispatch_policy'] = acu_dispatch_policy\n        state_updates.append({\n            'product_type_id': pt_id,\n            'description': build_description(original_description, updated),\n            'state': next_state,\n        })\n\n\ndef build_targets_stage_contract_smoke(queue_rows):\n    invalid = []\n    for idx, row in enumerate(to_list(queue_rows)):\n        if not isinstance(row, dict):\n            invalid.append({'index': idx, 'missing_fields': list(TARGETS_STAGE_CONTRACT_FIELDS)})\n            continue\n        missing = [field for field in TARGETS_STAGE_CONTRACT_FIELDS if field not in row]\n        if missing:\n            invalid.append({'index': idx, 'missing_fields': missing})\n    return {\n        'stage': 'targets',\n        'required_fields': list(TARGETS_STAGE_CONTRACT_FIELDS),\n        'queue_size': len(to_list(queue_rows)),\n        'ok': len(invalid) == 0,\n        'invalid_rows': invalid,\n    }\n\nreturn [{\n    'json': {\n        'run_subdomains': len(queue_wf_a_subdomains_pt) > 0,\n        'run_nmap': len(queue_wf_b_nmap_product) > 0,\n        'run_targets': len(queue_wf_c_targets_for_pt) > 0,\n        'run_acunetix': len(queue_wf_d_pt_acunetixscan) > 0,\n        'queue_wf_a_subdomains_pt': queue_wf_a_subdomains_pt,\n        'queue_wf_b_nmap_product': queue_wf_b_nmap_product,\n        'queue_wf_c_targets_for_pt': queue_wf_c_targets_for_pt,\n        'queue_wf_d_pt_acunetixscan': queue_wf_d_pt_acunetixscan,\n        'state_updates': state_updates,\n        'trigger_product_type_id': trigger_product_type_id,\n        'trigger_domain': trigger_domain,\n        'pt_window_size': pt_window_size,\n        'selected_pt_ids': sorted(list(selected_ids)),\n        'subdomains_concurrency': subdomains_limit,\n        'subdomains_running_now': subdomains_running_count,\n        'subdomains_running_jobs': subdomains_running_jobs,\n        'subdomains_slots_available': max(0, subdomains_limit - subdomains_running_jobs),\n        'subdomains_stale_recovered': recovered_stale_subdomains,\n        'subdomains_running_timeout_minutes': subdomains_stale_minutes,\n        'nmap_concurrency': nmap_limit,\n        'pt_lock_ttl_minutes': lock_ttl_minutes,\n        'lock_owner': run_lock_owner,\n        'retry_limits': retry_limits,\n        'stage_barriers': {\n            'subdomains_done_after_all_jobs': True,\n            'nmap_done_after_all_jobs': True,\n        },\n        'acu_dispatch_policy': acu_dispatch_policy,\n        'targets_stage_contract_smoke': build_targets_stage_contract_smoke(queue_wf_c_targets_for_pt),\n    }\n}]"
      },
      "id": "m-rebuild-plan",
      "name": "Rebuild Plan After WF_A (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        560,
        -140
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "first_item = _input.first()\npayload = first_item.json if first_item else {}\nupdates = payload.get('state_updates') if isinstance(payload, dict) else []\nif not isinstance(updates, list):\n    updates = []\nout = []\nfor u in updates:\n    if not isinstance(u, dict):\n        continue\n    pt_id = u.get('product_type_id')\n    desc = u.get('description')\n    if pt_id is None or desc is None:\n        continue\n    out.append({'json': {'product_type_id': pt_id, 'description': desc}})\nreturn out"
      },
      "id": "m-expand-state-updates",
      "name": "Expand PT State Updates (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -520,
        220
      ]
    },
    {
      "parameters": {
        "method": "PATCH",
        "authentication": "headerAuth",
        "url": "={{ ($env.DOJO_BASE_URL || 'http://localhost:8080/api/v2').replace(/\\/+$/,'') + '/product_types/' + $json.product_type_id + '/' }}",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { description: $json.description } }}",
        "options": {}
      },
      "id": "m-patch-pt-state",
      "name": "Patch PT State Description",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [
        -260,
        220
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "mGl4PbJkKfeJbTg8",
          "name": "Header Auth account"
        }
      },
      "retryOnFail": true,
      "maxTries": 3,
      "waitBetweenTries": 2000,
      "continueOnFail": true
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nimport re\nimport os\nfrom datetime import datetime, timezone\n\nBLOCK_START = \"PT_STATE_JSON_START\"\nBLOCK_END = \"PT_STATE_JSON_END\"\n\ndef read_key(container, key, default=None):\n    try:\n        val = container.get(key)\n        return default if val is None else val\n    except Exception:\n        try:\n            val = container[key]\n            return default if val is None else val\n        except Exception:\n            return default\n\ndef parse_state(description):\n    if not isinstance(description, str):\n        return {}\n    m = re.search(rf\"{BLOCK_START}\\n(.*?)\\n{BLOCK_END}\", description, re.DOTALL)\n    if not m:\n        return {}\n    try:\n        state = json.loads(m.group(1).strip())\n        return state if isinstance(state, dict) else {}\n    except Exception:\n        return {}\n\ndef build_description(base_description, state_payload):\n    text = base_description if isinstance(base_description, str) else ''\n    pattern = re.compile(rf\"\\n?{BLOCK_START}\\n.*?\\n{BLOCK_END}\\n?\", re.DOTALL)\n    clean = re.sub(pattern, \"\\n\", text).rstrip()\n    block = f\"{BLOCK_START}\\n{json.dumps(state_payload, ensure_ascii=False, sort_keys=True)}\\n{BLOCK_END}\"\n    if clean:\n        return clean + \"\\n\\n\" + block\n    return block\n\nitems = _input.all()\nif not items:\n    return []\n\nmax_retries = int(os.getenv('PT_RETRY_NMAP_MAX', '3') or 3)\nnow = datetime.now(timezone.utc).isoformat()\nby_pt = {}\nfor item in items:\n    row = item.json if isinstance(item.json, dict) else {}\n    try:\n        pt_id = int(read_key(row, 'product_type_id'))\n    except Exception:\n        continue\n\n    agg = by_pt.get(pt_id)\n    if agg is None:\n        agg = {\n            'description': read_key(row, 'nmap_state_description', ''),\n            'nmap_total': int(read_key(row, 'nmap_total', 0) or 0),\n            'nmap_done': int(read_key(row, 'nmap_done_initial', 0) or 0),\n            'nmap_failed': int(read_key(row, 'nmap_failed_initial', 0) or 0),\n            'done_inc': 0,\n            'failed_inc': 0,\n            'errors': [],\n        }\n        by_pt[pt_id] = agg\n\n    status = str(read_key(row, 'workflow_status', 'ok') or 'ok')\n    if status == 'ok':\n        agg['done_inc'] += 1\n    else:\n        agg['failed_inc'] += 1\n        err = str(read_key(row, 'error_message', '') or '').strip()\n        if err:\n            agg['errors'].append(err)\n\nout = []\nfor pt_id, agg in by_pt.items():\n    state_obj = parse_state(agg['description'])\n    counters = read_key(state_obj, 'counters', {})\n    if not isinstance(counters, dict):\n        counters = {}\n\n    nmap_total = agg['nmap_total'] if agg['nmap_total'] > 0 else int(counters.get('nmap_total', 0) or 0)\n    nmap_done = int(agg['nmap_done']) + int(agg['done_inc'])\n    nmap_failed = int(agg['nmap_failed']) + int(agg['failed_inc'])\n    processed_total = nmap_done + nmap_failed\n\n    counters['nmap_total'] = nmap_total\n    counters['nmap_done'] = nmap_done\n    counters['nmap_failed'] = nmap_failed\n\n    retry_count = int(read_key(state_obj, 'retry_count', 0) or 0)\n    last_error = read_key(state_obj, 'last_error')\n\n    if agg['failed_inc'] > 0:\n        retry_count += 1\n        diag = '; '.join([e for e in agg['errors'] if e][:2])\n        if not diag:\n            diag = f'nmap stage reported {agg[\"failed_inc\"]} failed job(s)'\n        last_error = f'NMAP error: {diag}'\n\n    all_nmap_jobs_processed = nmap_total == 0 or processed_total >= nmap_total\n    has_nmap_failures = nmap_failed > 0\n    has_pending_nmap_jobs = nmap_total > 0 and processed_total < nmap_total\n    retries_exhausted = retry_count >= max_retries\n    failure_mode = 'none'\n\n    if has_nmap_failures:\n        if retries_exhausted and not has_pending_nmap_jobs:\n            next_state = 'error'\n            failure_mode = 'terminal'\n        else:\n            next_state = 'nmap_running'\n            failure_mode = 'partial_retryable'\n    else:\n        next_state = 'nmap_done' if all_nmap_jobs_processed else 'nmap_running'\n        if next_state == 'nmap_done':\n            last_error = None\n\n    updated_state = {\n        'version': int(read_key(state_obj, 'version', 1) or 1),\n        'state': next_state,\n        'counters': counters,\n        'last_update': now,\n        'retry_count': retry_count,\n        'failure_mode': failure_mode,\n        'last_error': last_error,\n        'last_stage': 'nmap',\n        'lock_owner': None,\n        'lock_until': None,\n    }\n\n    out.append({'json': {\n        'product_type_id': pt_id,\n        'state': next_state,\n        'description': build_description(agg['description'], updated_state),\n        'workflow_status': 'ok' if next_state != 'error' else 'error',\n        'state_patch_status': 'state_patch_pending',\n        'state_patch_context': 'wf_b_summary',\n        'report_node': 'wf_b_summary',\n    }})\n\nreturn out\n"
      },
      "name": "Summarize WF_B Results (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1904,
        432
      ],
      "id": "m-summarize-b"
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.state_patch_context || ''}}",
              "operation": "equal",
              "value2": "wf_b_summary"
            }
          ]
        }
      },
      "id": "m-if-patch-from-wfb",
      "name": "Patch From WF_B Summary?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -60,
        220
      ]
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\n\nitems = _input.all()\nout = []\nfailed_total = 0\n\nfor item in items:\n    row = item.json if isinstance(item.json, dict) else {}\n    merged = dict(row)\n\n    has_error = bool(merged.get('error'))\n    merged['pt_id'] = merged.get('product_type_id')\n    merged['state_patch_status'] = 'state_patch_failed' if has_error else 'state_patch_ok'\n\n    merged['state_patch_api_response'] = merged.get('body')\n    if has_error and not merged.get('state_patch_api_response'):\n        merged['state_patch_api_response'] = merged.get('error')\n\n    if has_error:\n        failed_total += 1\n\n    out.append({'json': merged})\n\nfor item in out:\n    item['json']['state_patch_failed_total'] = failed_total\n\nreturn out\n"
      },
      "id": "m-finalize-state-patch-report",
      "name": "Finalize State Patch Report (Python)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        120,
        220
      ]
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$json.state_patch_status}}",
              "operation": "equal",
              "value2": "state_patch_ok"
            }
          ]
        }
      },
      "id": "m-if-state-patch-ok",
      "name": "State Patch OK?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        300,
        220
      ]
    },
    {
      "parameters": {
        "message": "={{ 'PT-state PATCH failed. pt_id=' + String($json.pt_id || $json.product_type_id || 'unknown') + '; context=' + String($json.state_patch_context || 'unknown') + '; status=' + String($json.statusCode || 'n/a') + '; api_response=' + JSON.stringify($json.state_patch_api_response || $json.body || $json.error || {}) }}"
      },
      "id": "m-stop-patch-error",
      "name": "Stop on PT State PATCH Error",
      "type": "n8n-nodes-base.stopAndError",
      "typeVersion": 1,
      "position": [
        500,
        300
      ]
    }
  ],
  "connections": {
    "Trigger": {
      "main": [
        [
          {
            "node": "Get Product Types",
            "type": "main",
            "index": 0
          },
          {
            "node": "Prepare Trigger Input (Python)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Products",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Products": {
      "main": [
        [
          {
            "node": "Prepare Products (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Plan (Python)": {
      "main": [
        [
          {
            "node": "Expand PT State Updates (Python)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Need WF_A_Subdomains_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_A_Subdomains_PT?": {
      "main": [
        [
          {
            "node": "Expand WF_A Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_B_Nmap_Product?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_A_Subdomains_PT": {
      "main": [
        [
          {
            "node": "Get Product Types After WF_A",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Products After WF_A",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_B_Nmap_Product?": {
      "main": [
        [
          {
            "node": "Expand WF_B Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_C_Targets_For_PT?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_B_Nmap_Product": {
      "main": [
        [
          {
            "node": "Summarize WF_B Results (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_C_Targets_For_PT?": {
      "main": [
        [
          {
            "node": "Expand WF_C Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Need WF_D_PT_AcunetixScan?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run WF_C_Targets_For_PT": {
      "main": [
        [
          {
            "node": "Need WF_D_PT_AcunetixScan?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Need WF_D_PT_AcunetixScan?": {
      "main": [
        [
          {
            "node": "Expand WF_D Candidates (Python)",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "Get Product Types": {
      "main": [
        [
          {
            "node": "Prepare Product Types (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_A Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_A_Subdomains_PT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_B Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_B_Nmap_Product",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_C Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_C_Targets_For_PT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand WF_D Candidates (Python)": {
      "main": [
        [
          {
            "node": "Run WF_D_PT_AcunetixScan",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Product Types (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare Trigger Input (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Products (Python)": {
      "main": [
        [
          {
            "node": "Merge Build Plan Input",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Trigger + Product Types": {
      "main": [
        [
          {
            "node": "Merge Build Plan Input",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge Trigger + Product Types After WF_A",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Build Plan Input": {
      "main": [
        [
          {
            "node": "Build Plan (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Product Types After WF_A": {
      "main": [
        [
          {
            "node": "Prepare Product Types After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Products After WF_A": {
      "main": [
        [
          {
            "node": "Prepare Products After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Product Types After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Merge Trigger + Product Types After WF_A",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare Products After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Merge Rebuild Plan Input",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Trigger + Product Types After WF_A": {
      "main": [
        [
          {
            "node": "Merge Rebuild Plan Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Rebuild Plan Input": {
      "main": [
        [
          {
            "node": "Rebuild Plan After WF_A (Python)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Rebuild Plan After WF_A (Python)": {
      "main": [
        [
          {
            "node": "Expand PT State Updates (Python)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Need WF_B_Nmap_Product?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Expand PT State Updates (Python)": {
      "main": [
        [
          {
            "node": "Patch PT State Description",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Summarize WF_B Results (Python)": {
      "main": [
        [
          {
            "node": "Patch PT State Description",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Patch PT State Description": {
      "main": [
        [
          {
            "node": "Patch From WF_B Summary?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Patch From WF_B Summary?": {
      "main": [
        [
          {
            "node": "Finalize State Patch Report (Python)",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "Finalize State Patch Report (Python)": {
      "main": [
        [
          {
            "node": "State Patch OK?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "State Patch OK?": {
      "main": [
        [
          {
            "node": "Need WF_C_Targets_For_PT?",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Stop on PT State PATCH Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "tags": []
}
